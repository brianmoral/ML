{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 - Linear Regression, Logistic Regression, and Feature Engineering\n",
    "## CSCI 5622 - Fall 2022\n",
    "***\n",
    "**Name**: Brian Morales\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11:59PM on October 28th**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas with the name format `PS3_<yourname>.ipynb`. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question. You can add a write-up markdown cell if it wasn't provided.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to previous homeworks.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import data\n",
    "import tests\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4556561023dddce9b62a1f66e65c735e",
     "grade": false,
     "grade_id": "linreg",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 1: Linear Regression (28 points)\n",
    "For this problem we will be using house price data from King County, Washington (Seattle area) for our linear regression implementations. We will be predicting house prices from the following features:\n",
    "\n",
    "| Feature | About |\n",
    "| --- | --- |\n",
    "| Bedrooms | Number of bedrooms |\n",
    "| Bathrooms | Number of bathrooms |\n",
    "| SQFT Living | Interior square footage |\n",
    "| SQFT Lot | Lot size |\n",
    "| Floors | Number of floors |\n",
    "| Waterfront | Whether the property overlooks water (1 if so, 0 otherwise) |\n",
    "| View | Rating of the quality of the view (0-4) |\n",
    "| Condition | Rating of the condition of the property (1-5) |\n",
    "| Grade | Rating of the quality of construction and design (1-13) | \n",
    "| SQFT Above | Square footage of interior space above ground level |\n",
    "| SQFT Basement | Square footage of interior space below ground level |\n",
    "| Year Built | Year first built |\n",
    "\n",
    "\n",
    "First, we want to scale the prices to the interval $[0, 1]$ using `MinMaxScaler`. Given an features matrix $X$, the scaling works as follows:\n",
    "\\begin{align}\n",
    "    X  \\leftarrow \\frac{X-min(X)}{max(X) - min(X)}\n",
    "\\end{align}\n",
    "\n",
    "this is performed as a column-wise operation (column = feature).\n",
    "\n",
    "As usual, $min(X)$ and $max(X)$ are computed on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70c3a1de57f97ec4920e6a6fa9ee23e5",
     "grade": false,
     "grade_id": "q11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.1.a **[2 points]** Complete `MinMaxScaler.fit` to compute and save the column-wise min and max.\n",
    "- 1.1.b **[1 points]** Complete `MinMaxScaler.transform` to apply the min-max scaling using the computed statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e00c45744385ef3d336b873915df4e",
     "grade": true,
     "grade_id": "a11",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute and save the features min and max of shape (num_features,)\n",
    "        (you can also save them with shape (1, num_features))\n",
    "        :param X: array of shape (num_samples, num_features)\n",
    "        :return: fitted scaler\n",
    "        \"\"\"\n",
    "        # Workspace 1.1.a\n",
    "        #BEGIN \n",
    "        self.max = np.amax(X, axis=0)\n",
    "        self.min = np.amin(X, axis = 0)\n",
    "        #END\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the given samples using the precomputed min and max\n",
    "        :param X: np.array of shape (num_samples, num_features)\n",
    "        :return: MinMax scaled X, of shape (num_samples, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace 1.1.b\n",
    "        #BEGIN \n",
    "        return (X - self.min)/(self.max - self.min)\n",
    "        #END\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit using X and then transform it. Useful when we need to scale just once.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "house_prices = data.HousePrices()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# MinMax works on 2-d arrays, so we just need to parse the prices as a single column/feature\n",
    "# and then squeeze it back to 1-d array\n",
    "house_prices.y_train = minmax_scaler.fit_transform(house_prices.y_train[:, None])[:, 0]\n",
    "house_prices.y_test = minmax_scaler.transform(house_prices.y_test[:, None])[:, 0]\n",
    "plt.hist(house_prices.y_train, bins=40)\n",
    "plt.title(\"Scaled prices distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9bce035031f7376f293d880cd67fc00",
     "grade": false,
     "grade_id": "q12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have seen that Ridge regression adds a regularization term to the least square using the L2 norm.\n",
    "Ridge regression is part of scikit-learn package ([read more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)), but we will be building our own implementation.\n",
    "You can test your implementation against scikit's.\n",
    "\n",
    "The objective of Ridge regression is to minimize:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_{i=1}^{N} ||y_i-\\mathbf{w}^T \\mathbf{x_i}-\\mathbf{b}||^2 + \\alpha||\\mathbf{w}||^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{w}$ denotes the coefficients vector for the linear regression model,\n",
    "$\\mathbf{b}$ the intercept vector, $\\alpha$ the trade-off regularization parameter, and $N$ the number of samples.\n",
    "\n",
    "Luckily for us, Ridge regression admits a closed form solution for $\\mathbf w$ and $\\mathbf{b}$.\n",
    "\n",
    "Let $X$ be the $N\\times d$ matrix whose rows are the training samples $(\\mathbf{x_i})_{i\\leq N}$ and $Y=(y_i)_{i\\leq N}$ the target values.\n",
    "\n",
    "First, we start by centering the features (columns of X) by subtracting the mean of the column to get centered matrix $\\tilde{X}$. Then the solution for the minimization is (trust me):\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbf{w^*} = (\\tilde{X}^T\\tilde{X}+ \\alpha I)^{-1}\\tilde{X}^TY \\\\\n",
    "&\\mathbf{b^*} = \\frac{1}{N} \\sum_{i}^{i=N} (y_i - \\mathbf{w^*}^T \\mathbf{x_i})\n",
    "\\end{align}\n",
    "\n",
    "Note that:\n",
    "- The samples in $b^*$ are not centered.\n",
    "- The predicted targets would be $\\hat{y}_i = \\mathbf{w^*}^T \\mathbf{x_i}+\\mathbf{b^*} $\n",
    "- $\\tilde{X}$ is only needed in the `fit` method when we compute $\\mathbf{w^*}$ and $\\mathbf{b^*}$\n",
    "\n",
    "A common practice is to scale or normalize (usually normalize) the features before fitting the model. It allows a _fair_ treatment of different features.\n",
    "You are allowed to use scikit's `StandardScaler` to do the normalization (yes, a thing called scaler is normalizing. Don't @ me, @ sklearn). Do not forget to run the same transformation on the test data before running the prediction.\n",
    "\n",
    "- 1.2 **[5 points]** Complete the `fit` and `evaluate` methods following the provided descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3], \n",
    "            [4,5,6],\n",
    "            [7,8,9,],\n",
    "            [8,3,11]])\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "905d77fd318287c1acefa7b60df8615e",
     "grade": true,
     "grade_id": "a12",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Ridge(object):\n",
    "\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha  # our tuning / regularization parameter\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w, norm of w)\n",
    "        \"\"\"\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.2.a\n",
    "        # TO DO: compute w and b and store them in self.coef_ and self.intercept\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        #BEGIN \n",
    "        if self.normalize: \n",
    "            self.scaler.fit(X)\n",
    "            x_hat = self.scaler.transform(X)\n",
    "        else: \n",
    "            x_hat = X \n",
    "        \n",
    "        x_hat = x_hat - np.mean(x_hat, axis=0)\n",
    "        \n",
    "        x_t = np.transpose(x_hat)\n",
    "        # this is the calculation in the paranthesis of w* \n",
    "        x_matrix = x_t @ x_hat + self.alpha * np.identity(X.shape[1])\n",
    "        x_inv = np.linalg.inv(x_matrix)\n",
    "        # storing w*\n",
    "        self.coefficients = x_inv @ x_t @ y\n",
    "        \n",
    "        num_nonzero_coefs = np.count_nonzero(self.coefficients)\n",
    "        coef_norm = np.linalg.norm(self.coefficients)\n",
    "        \n",
    "        # calculating b*\n",
    "        if self.normalize:\n",
    "            self.scaler.fit(X)\n",
    "            x_hat = self.scaler.transform(X)\n",
    "        else:\n",
    "            x_hat = X\n",
    "        w_on_x = np.transpose(self.coefficients)*x_hat\n",
    "        self.intercept = 1/X.shape[0] * np.sum(y - np.sum(w_on_x,  axis = 1))\n",
    "\n",
    "\n",
    "        #END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values of the test data\n",
    "        :param X: instances array of shape (num_samples, num_features)\n",
    "        :param y: the true targets, of shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.2.b\n",
    "        # TO DO: predict based on the test features and return the root mean squared error\n",
    "        #BEGIN \n",
    "        w_on_x = np.transpose(self.coefficients) * X\n",
    "        y_pred = np.sum(w_on_x, axis= 1) + self.intercept\n",
    "        rmse = np.sqrt((np.sum((y - y_pred)**2))/len(y))\n",
    "\n",
    "        #END\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.2.a: [PASS]\n",
      "Question 1.2.a: [PASS]\n",
      "Question 1.2.a: [PASS]\n",
      "Question 1.2.a: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Tests cells, do not remove\n",
    "# Should run without errors\n",
    "tests.test_ridge_coef(Ridge, normalize=False)\n",
    "tests.test_ridge_coef(Ridge, normalize=True)\n",
    "tests.test_ridge_intercept(Ridge, normalize=True)\n",
    "tests.test_ridge_intercept(Ridge, normalize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b41a1efdc8c9b88c0be8ba2a7335354",
     "grade": false,
     "grade_id": "q13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.3 **[3 points]** Produce 3 plots as a function of $\\log_{10}(\\alpha)$ (logarithmic scale) that compare normalized versus non-normalized Ridge:\n",
    "    - The number of non-zero coefficients of $\\mathbf w$\n",
    "    - The norm of $\\mathbf w$\n",
    "    - The test RMSE (Root Mean Squared Error)\n",
    "\n",
    "Use the values of $\\alpha$ provided in the cell. What is the best `alpha` for each version of the two models?\n",
    "\n",
    "To produce multiple plots in the same figure, see the examples [here](https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5a7f4ac64ab04983d323faa2db0662",
     "grade": true,
     "grade_id": "a13",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3, 1.0, 10.0, 100.0, 300.0, 500.0, 1e3, 1.5e3, 2e3, 5e3, 1e4]\n",
    "num_nonzero_coef_arry = np.zeros(len(alphas))\n",
    "num_nonzer_coef_non_arry = np.zeros(len(alphas))\n",
    "rmse_arry = np.zeros(len(alphas))\n",
    "# Workspace 1.3\n",
    "#BEGIN \n",
    "for index, alpha in enumerate(alphas):\n",
    "    norm_R = Ridge(np.log(alpha), normalize=True)\n",
    "    non_norm_R = Ridge(np.log(alpha), normalize = False)\n",
    "    num_coef, norm_w = norm_R.fit(house_prices.X_test, house_prices.y_test)\n",
    "    num_nonzer_coef_non , non_norm_w = non_norm_R.fit(house_prices.X_test, house_prices.y_test)\n",
    "    rmse = norm_R.evaluate(house_prices.X_test, house_prices.y_test)\n",
    "    rmse = non_norm_R.evaluate(house_prices.X_test,house_prices.y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87e590454616cedacebd16e12fee8b16",
     "grade": true,
     "grade_id": "a13b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Workspace 1.3b\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9fe7f5b0084b06469c94e83a8eb2ae1",
     "grade": false,
     "grade_id": "q14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the results from previous questions, you should have noticed that the interpretation of Ridge regression is not an easy task.\n",
    "One way to solve that is to use a regularization that adds _sparsity_ to $\\mathbf w$ and excludes less important features.\n",
    "That's what Lasso regression is about.\n",
    "\n",
    "Lasso uses $l_1$ norm in the regularization term and minimizes:\n",
    "\\begin{align}\n",
    "\\frac{1}{2N}\\sum_i ||y_i-\\mathbf{w}^T \\mathbf{x}_i -\\mathbf{b}||^2 + \\alpha||\\mathbf{w}||_1\n",
    "\\end{align}\n",
    "\n",
    "It is part of scikit package ([more details](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html))\n",
    "and you're free to compare your implementation to scikit's.\n",
    "\n",
    "Unfortunately, there is no closed form solution for Lasso. Instead, we have powerful algorithms to optimize it.\n",
    "We will be using lasso-path solver from scikit-learn for our implementation to find $\\mathbf w^*$\n",
    "([see more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path)).\n",
    "(Note that `lasso_path` takes a list of $\\alpha$ in the argument `alphas` and not a scalar.)\n",
    "\n",
    "Similar to Ridge's, $ \\mathbf w^*_{lasso} $ is computed by feeding the centered $\\tilde{X}$ to `lasso_path`.\n",
    "Once $\\mathbf w^*$ is found, $\\mathbf{b}^*$ has the same formula from Ridge regression.\n",
    "\n",
    "- 1.4 **[4 points]** Complete the Lasso class in the same way Ridge class was created. (You're allowed to use inheritance for a more concise code, as long as the class has `coefficients` and `intercept` attributes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "082eefa7cee4a116bad8797f18497236",
     "grade": true,
     "grade_id": "a14",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "\n",
    "class Lasso(object):\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "        self.alpha = alpha  # our tuning / regularization parameter\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the lasso model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w: scalar, norm of w: scalar)\n",
    "        \"\"\"\n",
    "\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.4.a\n",
    "        # TO DO: compute w and b and store then in self.coef_ and self.intercept\n",
    "        # TO DO: call lasso_path on the centered features to compute self.coef_\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values  of the test data\n",
    "        :param X: features array, shape (num_samples, num_features)\n",
    "        :param y: true targets, shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "        root_mean_squared_error = 0\n",
    "        # Workspace 1.4.b\n",
    "        # TO DO: predict based on the test features and return the mean_squared_error\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tests cell, do not remove\n",
    "# Test non-normalized Lasso\n",
    "tests.test_lasso_coef(Lasso, normalize=True)\n",
    "tests.test_lasso_coef(Lasso, normalize=False)\n",
    "tests.test_lasso_intercept(Lasso, normalize=True)\n",
    "tests.test_lasso_intercept(Lasso, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d98b0ff3a942b3ea4b00ca2b89eaa93",
     "grade": false,
     "grade_id": "q15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.5 **[3 points]** Produce 3 plots as a function of $\\log_{10}(\\alpha)$ (logarithmic scale) that compare normalized versus non-normalized Lasso:\n",
    "    - The number of non-zero coefficients of $\\mathbf w$\n",
    "    - The norm of $\\mathbf w$\n",
    "    - The test RMSE (Root Mean Squared Error)\n",
    "\n",
    "Use the values of $\\alpha$ provided in the cell. What is the best `alpha` for each version of the two models?\n",
    "\n",
    "Can you get all coefficients of $\\mathbf w$ to 0 for the non-normalized Lasso?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1023b8532f36cc21c11adeff6d52494b",
     "grade": true,
     "grade_id": "a15",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.2]\n",
    "# Workspace 1.5\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf4a686b42cb9de59e65dca6ca442cb4",
     "grade": true,
     "grade_id": "a15b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Workspace 1.5b\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbe2b8623c92d9fde890c4755a5fdc26",
     "grade": false,
     "grade_id": "q16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 1.6 **[3 points]** (Write-up) Compare the two algorithms on the house prices dataset: compare the number of non-zero coordinates of Ridge vs Lasso and their RMSE on each dataset. Which type of regression is better? When does normalization improve the RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16e3e96e6a8a736508c94894ba481731",
     "grade": true,
     "grade_id": "a16",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Workspace 1.6\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e2eb13340eb36c410f8e1937598bd53",
     "grade": false,
     "grade_id": "q17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lasso is expected to have higher level of sparsity compared to Ridge. On the other hand, when the number of features is very large\n",
    "(larger than the sample size), Lasso can struggle at picking the right features.\n",
    "Elastic Net try to combine both types of regularization to get the best of both worlds by minimizing:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{2N} \\sum_i ||y_i - \\mathbf{w}^T\\mathbf{x}_i -\\mathbf{b}||^2_2 + \\alpha\\beta||\\mathbf{w}||_1 + \\frac{\\alpha}{2}(1 - \\beta)||\\mathbf{w}||^2_2\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta\\in[0,1]$ is the $l_1$ ratio ($\\beta=1$ for Lasso and $\\beta=0$ for Ridge).\n",
    "\n",
    "We'll be using [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) solver from scikit.\n",
    "\n",
    "- 1.7 **[2 points]** Complete ElasticNet regression class `Elastic` by implementing fit and evaluate methods with the same signature as Lasso and Ridge. (Again, you can use inheritance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dd6f5fc8561b82e7eee657f7d0c1f8b",
     "grade": true,
     "grade_id": "a17",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "class Elastic(object):\n",
    "    def __init__(self, alpha, beta=0.7, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param beta: l1_ratio for ElasticNet\n",
    "        :param normalize: normalization flag\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.normalize = normalize\n",
    "        self.scaler = StandardScaler()\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        self._model = ElasticNet(alpha=alpha, l1_ratio=beta)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit self._model using the provided data and store the coef and intercept in self.coef_ and self.intercept\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (n_samples, n_features)\n",
    "        :param y: target values (n_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w, norm of w)\n",
    "        \"\"\"\n",
    "\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.7.a\n",
    "        # TO DO: Complete Elastic Net\n",
    "        # TO DO: save w and b in self.coef_ and self.intercept respectively\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values  of the test data\n",
    "        :param test_x: test features\n",
    "        :param test_y: test target\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "\n",
    "        root_mean_squared_error = 0\n",
    "        # TO DO: predict based on the test features and return the mean_squared_error\n",
    "        # Workspace 1.7.b\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tests cell, do not remove\n",
    "tests.test_elastic_coef(Elastic, normalize=False)\n",
    "tests.test_elastic_coef(Elastic, normalize=True)\n",
    "tests.test_elastic_intercept(Elastic, normalize=False)\n",
    "tests.test_elastic_intercept(Elastic, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fba05394f02cd7095d75bef208ddb355",
     "grade": false,
     "grade_id": "q18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.8 **[2 points]** Produce 3 plots as a function of $\\log_{10}(\\alpha)$ (logarithmic scale) that compare normalized versus non-normalized `Elastic`:\n",
    "    - The number of non-zero coefficients of $\\mathbf w$\n",
    "    - The norm of $\\mathbf w$\n",
    "    - The test RMSE (Root Mean Squared Error)\n",
    "\n",
    "Use the values of $\\alpha$ provided in the cell, and $\\beta= 0.3$. What is the best `alpha` for each version of the two models? How does it compare to the previous models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf4146692776f2134d193594c623574",
     "grade": true,
     "grade_id": "a18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1e-4, 1e-3, 1e-2, 0.02, 0.05, 0.1, 0.2, 0.3, 1.0, 2.0, 10.0]\n",
    "# Workspace 1.2\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08961fe33266754a6159026a7bece0f1",
     "grade": true,
     "grade_id": "a18b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Workspace 1.8.b\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef83a2a530c7d641569eb7e24a59be8b",
     "grade": false,
     "grade_id": "q19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1.9 **[3 points]** What are the cons and pros of each of the three types of regressions we have tested?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09afc142fe1eac8c9194ebfec60a357a",
     "grade": true,
     "grade_id": "a19",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Workspace 1.9\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "121953e598fbe84a7427016f10b6935d",
     "grade": false,
     "grade_id": "problem2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "## Problem 2: Logistic Regression for Binary Classification (30 points)\n",
    "\n",
    "The second part of this assignment will be dealing with Logistic Regression.\n",
    "While the name \"regression\" suggests otherwise, Logistic Regression is actually used for classification.\n",
    "It's a regression problem because the targets are the continuous likelihoods of the outcomes.\n",
    "\n",
    "Our dataset is the same one we used in Problem Set 1. We'll start with a binary Logistic Regression. So our class label is `0` if the digit is even and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_digits = data.BinaryDigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5e218b3c7a406621501cd3432ebdd54",
     "grade": false,
     "grade_id": "nll",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the same notations from Problem 1 (this time with discrete targets $y_i$ in {0, 1}), _Logistic Regression_ is about minimizing the **N**egative **L**og **L**ikelihood objective defined as:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(\\mathbf{w}) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(\\mathbf{w}^T{\\mathbf{x_i}}) + (1-y_i)\\log(1 - \\sigma(\\mathbf{w}^T\\mathbf{x_i}))\\right]\n",
    "\\end{align}\n",
    "\n",
    "You might be wondering: where is the intercept?\n",
    "We're including the intercept in $(\\mathbf x_i)_i$ since we'll be adding a constant feature $\\mathbf x_{i,0} = 1$ to all samples $(\\mathbf x_i)_i$.\n",
    "We will call it the zero-th column and the intercept will be $\\mathbf{w}_0$.\n",
    "This zero-th column will be appended to the training samples in `fit` method and the test samples in `predict` method. $\\sigma$ is the sigmoid function seen in class.\n",
    "\n",
    "The gradient of **NLL** w.r.t $\\mathbf{w}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\textrm{NLL}}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_i \\left[\\sigma(\\mathbf{w}^T\\mathbf{x}_i)-y_i)\\right]\\mathbf{x}_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f95c51d954665df0971edef4cafcf0f",
     "grade": false,
     "grade_id": "q21_25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 2.1 **[3 points]** Complete the `sigmoid` function to return the sigmoid values $\\sigma(\\mathbf{w}^T{\\mathbf{x_i}})$ given features array. You have to truncate the score $\\mathbf{w}^T{\\mathbf{x_i}}$ to the interval $[-25, 25]$ to avoid overflow of `np.exp`\n",
    "\n",
    "\n",
    "- 2.2 **[3 points]** Finish the `compute_gradient` function to return the derivative of the cost w.r.t. the weights\n",
    "\n",
    "- 2.3 **[3 points]** Finish the `batch_update` function so that it performs batch gradient descent using the provided batch data\n",
    "\n",
    "- 2.4 **[4 points]** Finish the `fit` function so that it performs several training epochs and returns the Recall score on the validation data at the end of each epoch. Initialize $\\mathbf{w}$ so that $\\mathbf{w}_j = \\frac{1}{d}$\n",
    "\n",
    "- 2.5 **[2 points]** Finish the `predict` method to return the predicted labels (either 0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2caf5fc6ded761f14377970f1a5b6455",
     "grade": true,
     "grade_id": "a21_25",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, eta=0.1, alpha=0):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param eta: Learning rate\n",
    "        :param alpha: We will use this parameter later (IN BONUS)\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = None  # uninitialized w\n",
    "        self.eta = eta  # learning rate\n",
    "        self.initialized = False # flag used to initialize w only once, it allows calling fit multiple times\n",
    "        self.alpha = alpha  # regularization / penalty term (USED IN BONUS)\n",
    "\n",
    "    def sigmoid(self, x, threshold=25.0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: features array of shape (num_samples, num_features + 1) (zero-th column appended)\n",
    "        :param threshold: the truncating threshold for np.exp, default to 25.0\n",
    "        :return: sigmoid values , of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.1\n",
    "        # TO DO: Complete this function to return the output of applying the sigmoid function to the score\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the derivative of the cost w.r.t to the weights. Don't forget to average by batch_size\n",
    "        :param x:  Feature vector, shape (batch_size, num_features +1), with zero-th column appended\n",
    "        :param y: real binary class label, shape (batch_size)\n",
    "        :return: gradient of shape (num_features + 1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.2\n",
    "        # TO DO: Finish this function to compute the gradient\n",
    "        gradient = np.zeros((x.shape[1], ))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return gradient\n",
    "\n",
    "    def batch_update(self, batch_x, batch_y):\n",
    "        \"\"\"\n",
    "        Single self.w update using the batch.\n",
    "        :param batch_x: array of features (includes the constant feature at column 0), of shape (batch_size, num_features + 1)\n",
    "        :param batch_y: array of target values, shape (batch_size,)\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.3\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=1, validation_X=None, validation_y=None):\n",
    "        \"\"\"\n",
    "        train the LogisticRegression\n",
    "        :param X: training features, shape (num_samples, num_features)\n",
    "        :param y: training labels, shape (num_samples,)\n",
    "        :param epochs: number of epochs, integer\n",
    "        :param batch_size: size of batch for gradient update, 1 for SGD\n",
    "        :param validation_X: validation rows, should default to training data if not provided\n",
    "        :param validation_y: validation labels\n",
    "        :return: recall score at the end of each epoch on validation data\n",
    "        \"\"\"\n",
    "\n",
    "        if validation_X is None:\n",
    "            validation_X, validation_y = X, y\n",
    "        metrics = []\n",
    "        # Workspace  2.4\n",
    "        # TO DO: Process X to append the zero-th constant column and call self.optimize\n",
    "        # TO DO: Compute average recall on the validation data at the end of each epoch\n",
    "        # HINT: make sure to initialize w\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return np.array(metrics)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: features array, shape (num_samples, num_features) (without the constant column)\n",
    "        :return: predicted binary label, shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.5\n",
    "        y_hat = np.zeros((X.shape[0],))\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # We append zero-th column\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return y_hat\n",
    "\n",
    "    def optimize(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Perform one epoch batch gradient on shuffled data\n",
    "        :param X: np.array of shape (num_samples, num_features +1), The training data with zero-th column appended\n",
    "        :param y: target values of shape (num_samples,)\n",
    "        :param batch_size: batch_size of the batch_update\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        indices = np.random.permutation(len(X))\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            batch_x = X[indices[i:i + batch_size]]\n",
    "            batch_y = y[indices[i:i + batch_size]]\n",
    "            self.batch_update(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b966586e4d91d8169a392bbd579bc05",
     "grade": false,
     "grade_id": "q26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing the class above, loop over the training data and perform batch training with `batch_size=1`(Stochastic Gradient Descent) for 50 epochs, and different values of eta ( see `etas`). Train your model and do the following:\n",
    "\n",
    "- 2.6 **[5 points]** Plot the recall trend for the different values of eta on the training data (epoch vs recall). (reset random seed for each loop to mitigate the randomness effect)\n",
    "\n",
    "Use the values provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44a06f9b3d581c2badaea070909732bd",
     "grade": true,
     "grade_id": "a26",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "etas = [.001, .01, 0.1, 1]\n",
    "\n",
    "for eta in etas:\n",
    "    np.random.seed(2022)  # Reset randomness\n",
    "    # Workspace 2.6\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81ea96b6a187d0092f518679285de42b",
     "grade": false,
     "grade_id": "q27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 2.7 **[3 points]** Plot the recall trend for the different values of eta on the test data.\n",
    "(reset random seed for each loop to mitigate the randomness effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1ea471ead529000013910a78764246a",
     "grade": true,
     "grade_id": "a27",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "etas = [.0001, .001, .01, 1]\n",
    "\n",
    "for eta in etas:\n",
    "    np.random.seed(2022)\n",
    "    # Workspace 2.7\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a1226541d770401670fbd26ee0c1892",
     "grade": false,
     "grade_id": "q28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time we want to analyze the effect of varying the batch size. We fix `eta=0.01` and `epochs=50` and we want to\n",
    "examine the recall on the test set at the end of the training.\n",
    "\n",
    "- 2.8 **[4 points]** Produce a plot of the recall on the test data at the end of the training as a function of the batch size. Reset the random generator for each iteration.\n",
    "\n",
    "Use batch sizes provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "591e3a4fcdfda30e94bc59fb554e2159",
     "grade": true,
     "grade_id": "a28",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = list(range(1, 40))\n",
    "recalls = []\n",
    "\n",
    "for b_size in batch_sizes:\n",
    "    np.random.seed(2022)\n",
    "    # Workspace 2.8\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "plt.plot(batch_sizes, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43f43647e5ecd1ec7b57d98ba19b9529",
     "grade": false,
     "grade_id": "q29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2.9 **[3 points]** How does the learning rate (eta) and the number of epochs affect the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0792329373f012c1f7c952dd938b67b6",
     "grade": true,
     "grade_id": "a29",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Workspace 2.9\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fb2f573735bec6ae5c54a6f2382dc94",
     "grade": false,
     "grade_id": "q210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.10 (**Bonus**) **[4 points]** Since we're done with the binary regression, we will try to add Ridge regularization:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(\\mathbf{w}) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(\\mathbf{w}^T{\\mathbf{x}_i}) + (1-y_i)\\log(1 - \\sigma(\\mathbf{w}^T\\mathbf{x}_i))\\right] + \\alpha {||\\mathbf{w}||^2}_{\\geq1}\n",
    "\\end{align}\n",
    "\n",
    "This is exactly what we will be using the `alpha` parameter in `LogisticRegression` for.\n",
    "First, write the gradient formula in the cell below and edit your `compute_gradient` to account for the regularization term. Note that the regularization $||\\mathbf{w}||^2$ does not apply to the intercept $\\mathbf{w}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c714d2e2a69be974365824a9558760bd",
     "grade": true,
     "grade_id": "a210",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Workspace 2.10\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67c9dc01cb019a680f2d971dbc5da8c5",
     "grade": false,
     "grade_id": "cell-2b53d5d40987324e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 3: Logistic Regression for Multiclass Classification (24)\n",
    "\n",
    "You will now create a classifier that is commonly referred to as _One Versus All_ Logistic Regression,\n",
    "where a binary classifier will be trained to identify a single class. The prediction is performed by assigning the label\n",
    "of the classifier that is most confident in its prediction (the highest score/sigmoid value).\n",
    "\n",
    "The dataset will be the same one we use in Problem Set 1's KNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "303f1aecde55d7dc0f46626cc6887659",
     "grade": false,
     "grade_id": "digits",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "digits = data.Digits()\n",
    "scaler = StandardScaler()\n",
    "digits.X_train = scaler.fit_transform(digits.X_train)\n",
    "digits.X_test = scaler.transform(digits.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fa990a93b4df2da524da310c7facadf",
     "grade": false,
     "grade_id": "q31_32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 3.1 **[3 points]** Complete `one_hot_encoding` to transform your integer labels into a set of binary features via one-hot encoding.\n",
    "You can assume that it's called after `label_to_index` and `index_to_label` have been defined.\n",
    "\n",
    "\n",
    "- 3.2 **[2 points]** Complete `normalize_data` to normalize your features. You can use `StandardScaler`\n",
    "- 3.3 **[4 points]** Complete `predict` method to return the predicted label using the scores from each class classifier.\n",
    "You might be tempted to call `LogisticRegression.predict`, but you need the sigmoid values to determine the most likely label (and hence have to add the zero-th column in OneVersusAll).\n",
    "\n",
    "- 3.4 **[2 points]** Complete `evaluate` to return the weighted average recall (WAR) on the validation data (use `recall_score`)\n",
    "- 3.5 **[6 points]** Complete `fit` function to fit all classifiers with their corresponding labels and compute the WAR at the end of each epoch\n",
    "\n",
    "Reminder: make no assumption about the number of features/classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc67e8cd7de1323ccab8f6a298779b82",
     "grade": true,
     "grade_id": "a31_32",
     "locked": false,
     "points": 17,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OneVersusAll(object):\n",
    "\n",
    "    def __init__(self, eta=0.1, alpha=0):\n",
    "        self.eta = eta  # Learning Rate\n",
    "        self.classifiers = []  # Array of LogisticRegression classifiers\n",
    "        self.alpha = alpha  # regularization / penalty rate (USED IN BONUS)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False  # Flag to store whether the model is already fitted to avoid fitting scaler more than once\n",
    "\n",
    "    def one_hot_encoding(self, y):\n",
    "        \"\"\"\n",
    "        Create one-hot encoding of y\n",
    "        :param y: shape (num_samples,)\n",
    "        :return: one hot encoding (num_samples, C) where C is the number of classes in the training labels\n",
    "        \"\"\"\n",
    "        one_hot_encode = np.zeros((y.shape[0], len(self.label_to_index)), dtype=int)\n",
    "        # Workspace 3.1\n",
    "        # TO DO: Represent the output vector y as a one hot encoding. Create a matrix M of dimensions (n X C)\n",
    "        # where n = number of samples, and C for number of classes\n",
    "        # If y[i] = class j, then M[i] is equal to 1 at position j and 0 elsewhere\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return one_hot_encode\n",
    "\n",
    "    def normalize_data(self, X):\n",
    "        \"\"\"\n",
    "        Fit scaler if it hasn't been done yet and normalize X\n",
    "        :param X: Raw features, shape (num_samples, num_features)\n",
    "        :return: Normalized features,  shape (num_samples, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace 3.2\n",
    "        # TO DO: Normalize the feature values of dataset X, you have to keep track of the mean and variance\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The usual predict method\n",
    "        \"\"\"\n",
    "        # Workspace 3.3\n",
    "        # TO DO: Normalize X, process it and return the predicted label using the scores from classifiers\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        # Workspace 3.4\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y, batch_size=1, epochs=10, validation_X=None, validation_y=None):\n",
    "        \"\"\"\n",
    "        Similar to LogisticRegression, but for multiclass case\n",
    "        :return: metrics, containing WAR score at the end of each epoch on the validation data\n",
    "        \"\"\"\n",
    "        self.label_to_index = dict(zip(np.unique(y), range(len(np.unique(y)))))\n",
    "        self.index_to_label = {v: k for k, v in self.label_to_index.items()}\n",
    "\n",
    "        metrics = []\n",
    "        if validation_X is None:\n",
    "            validation_X, validation_y = X, y\n",
    "        # Workspace 3.5\n",
    "        # TO DO: Compute Weighted Average Recall at the end of each epoch\n",
    "        # TO DO: Process training data and use it to fit self.classifiers and compute the metrics every epoch\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd3a3693333028fa76f5d2df5e6b638d",
     "grade": false,
     "grade_id": "q36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 3.6 **[4 points]** Train `OneVersusAll` for 100 epochs using eta=0.1 and batch_size = 64. In the same figure, plot:\n",
    "    - The WAR metric on the digits training data at the end of each epoch\n",
    "    - The WAR metric on the digits test data at the end of each epoch\n",
    "You should also print the best score on each if the partitions.\n",
    "You might need to adapt LogisticRegression so that fit method can be called multiple times without losing the weights from previous epoch.\n",
    "\n",
    "For a fair comparison, call `np.random.seed(42)` before each `fit` to reset the random generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40fb5965a3d38e191d3e38e3ca640c2c",
     "grade": true,
     "grade_id": "a36",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "eta = 0.1\n",
    "batch_size = 64\n",
    "# Workspace 3.6\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a78fad40c4dce28db009c65ef7fa01b",
     "grade": false,
     "grade_id": "q37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 3.7 **[3 points]** Does the WAR improve as we train for more epochs on the two data partitions (train, test)? Why? How does it compare to KNNClassifier? How do you explain the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d60925a54a8c60fb20b065886b8f85",
     "grade": true,
     "grade_id": "a37",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "% Workspace 3.7\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2aae0ab222bcd47033ed6adebee0df7",
     "grade": false,
     "grade_id": "q38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 3.8 **(Bonus)** **[3 points]** Repeat 3.6 and 3.7 using Ridge regularization for the LogisticRegression classifiers with alpha=0.01. How does the regularization affect the performance on train VS test sets?\n",
    "\n",
    "You have to answer 2.10 before you attempt this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "009492acfacf8cbc85eddfce91dd73ae",
     "grade": true,
     "grade_id": "a38",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "eta = 0.1\n",
    "batch_size = 64\n",
    "alpha = 1e-2\n",
    "# Workspace 3.8 (code)\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b3622c38cddf15acd78e79c344bd300",
     "grade": true,
     "grade_id": "a38b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Workspace 3.8 (write up)\n",
    "\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a480252677e8fdd3a4579ddfc0590f54",
     "grade": false,
     "grade_id": "P4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 4: Feature Engineering (18 points)\n",
    "So far in all previous problems, we didn't have to worry about creating the features, that is our `X`. We always relied on having it handed to us.\n",
    "In practical machine learning problems, however, you will have to do some feature engineering yourself in order to get the features matrix `X` before feeding it to the classifier.\n",
    "\n",
    "For this problem you will work on extracting features from raw data. You will then use scikit-learn [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) to complete the classification task.\n",
    "\n",
    "We will start by importing some libraries and loading our data which contains 2,000 reviews from IMDB along with their sentiment, either positive (`1`) or negative (`0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = data.IMDB()\n",
    "print(f\"[Example: {imdb.X_train[1][:157]}], label:{imdb.y_train[1]}\")"
   ]
  },
  {
   "attachments": {
    "Pipeline.png": {
     "image/png": "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"
    },
    "Union.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call _Featurizer_ any class that is implemented like `StandardScaler` and our initial `MinMaxScaler`. It has:\n",
    "- a `fit` method that computes any necessary statistics based on our training data (like mean, min or max)\n",
    "- a `transform` method that generates new features based on given input and the pre-computed statistics\n",
    "- a `fit_transform` method that combines the two previous methods\n",
    "\n",
    "We can also combine multiple _Featurizers_ either in a series using [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):\n",
    "![Pipeline.png](attachment:Pipeline.png)\n",
    "\n",
    "Or in parallel using [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    "![Union.png](attachment:Union.png)\n",
    "\n",
    "`Pipeline` and `FeatureUnion` are also _Featurizers_.\n",
    "\n",
    "Both `Pipeline` and `FeatureUnion` take a list of tuples formated as `(features_name: str, featurizer_instance: object)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6959d629e661cdbbaa7346a21602d58c",
     "grade": false,
     "grade_id": "q41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by extracting some simple features such as the number of sentences in each review.\n",
    "To do that, we will create a custom scikit-learn feature transformer which will take data as input and returns a feature vector as output.\n",
    "\n",
    "The feature transformer class has a constructor, a `fit` method. The constructor can be used to store any words or other data that we might need during the feature extraction process.\n",
    "Feature transformers usually don't need to be fitted, so we will leave that as it is. Most of the work will be done inside the `transform` function.\n",
    "\n",
    "- 4.1 **[3 points]** Complete the `SentenceCounter` class to extract the number of sentences in each review. \n",
    "- 4.2 **[3 points]** Complete the `PunctCounter` class to count the number of \"!\" and \"?\" in each review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5175aa8874a97ceb9af162cb681490e",
     "grade": true,
     "grade_id": "a41",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SentenceCounter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom feature transformer to extract the number of sentences\n",
    "    \"\"\"\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        :param data: raw data, of length (num_samples,), where each element is a string \n",
    "        :return: features array of shape (num_samples, 1)\n",
    "        \"\"\"\n",
    "        features = np.zeros((len(data), 1))\n",
    "        # Workspace 3.1\n",
    "        # TODO: Transform reviews into the feature (number of sentences)  \n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return features\n",
    "\n",
    "\n",
    "class PunctCounter(SentenceCounter):\n",
    "    \"\"\"\n",
    "    A custom feature transformer to extract the number of \"!\" and \"?\"\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        :param data: raw data, of length (num_samples,), where each element is a string \n",
    "        :return: features array of shape (num_samples, 1)\n",
    "        \"\"\"\n",
    "        features = np.zeros((len(data), 1))\n",
    "        # Workspace 3.2\n",
    "        # TODO: Transform reviews into the feature (number of \"!\" and \"?\")  \n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `featurizer` instance below combines `FeatureUnion` and `Pipeline` operations. First, we extract the number of punctuations, then normalize the two features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Pipeline([(\"counts\", FeatureUnion([('sentences_count', SentenceCounter()),\n",
    "                                                ('punct_count', PunctCounter())])),\n",
    "                       (\"normalizer\", StandardScaler())])\n",
    "X_train = featurizer.fit_transform(imdb.X_train)\n",
    "X_test = featurizer.transform(imdb.X_test)\n",
    "print(f\"means:{np.mean(X_train,axis=0)}, Standard deviations:{np.std(X_train, axis=0)}\")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this simple feature would help us distinguish positive reviews from negative ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier and find the accuracy on the test dataset\n",
    "lr = SGDClassifier(loss='log', penalty='l2', alpha=0.001, max_iter=2000, shuffle=True, verbose=0, random_state=42)\n",
    "lr.fit(X_train, imdb.y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, imdb.y_test)\n",
    "print(\"Accuracy on testing set: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78850abfab8723fca912d015faf934cf",
     "grade": false,
     "grade_id": "q43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Well, that was expected! let's create more transformers to extract more valuable features.  \n",
    "\n",
    "- 4.3 **[3 points]** Complete the `NegativeCounter` feature transformer to extract the number of negative words (vocabulary is provided) \n",
    "\n",
    "- 4.4 **[3 points]** Complete the `PositiveCounter` feature transformer to extract the number of positive words (vocabulary is provided)\n",
    "\n",
    "HINT: Before you match the vocabulary words to the review text, use [`word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html) from `nltk` to convert reviews into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e0b55e99f4ad85c38ed1bf0f074cdd6",
     "grade": true,
     "grade_id": "a43",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NegativeCounter(SentenceCounter):\n",
    "    \"\"\"\n",
    "    A custom feature transformer to extract the feature \"number of negative words\" \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.neg_words = [\"second-rate\", \"violent\", \"moronic\", \"third-rate\", \"flawed\", \"juvenile\", \"boring\",\n",
    "                          \"distasteful\", \"ordinary\", \"disgusting\", \"senseless\", \"static\", \"brutal\", \"confused\",\n",
    "                          \"disappointing\", \"bloody\", \"silly\", \"tired\", \"predictable\", \"stupid\", \"uninteresting\",\n",
    "                          \"weak\", \"incredibly tiresome\", \"trite\", \"uneven\", \"clich ridden\", \"outdated\",\n",
    "                          \"dreadful\", \"bland\", \"bad\", \"worst\", \"waste\"]\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        :param data: raw data\n",
    "        :return: features array of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        features = np.zeros((len(data), 1))\n",
    "\n",
    "        # Workspace 4.3\n",
    "        # TODO: Transform reviews into the feature (number of negative words)  \n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class PositiveCounter(NegativeCounter):\n",
    "    \"\"\"\n",
    "    A custom feature transformer to extract the feature \"number of positive words\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pos_words = [\"first-rate\", \"insightful\", \"clever\", \"charming\", \"comical\", \"charismatic\", \"enjoyable\",\n",
    "                          \"uproarious\", \"original\", \"tender\", \"hilarious\", \"absorbing\", \"sensitive\", \"riveting\",\n",
    "                          \"intriguing\", \"powerful\", \"fascinating\", \"pleasant\", \"surprising\", \"dazzling\",\n",
    "                          \"imaginative\", \"legendary\", \"unpretentious\", \"love\", \"wonderful\",\n",
    "                          \"best\", \"great\", \"superb\", \"still\", \"beautiful\"]\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        :param data: raw data\n",
    "        :return: features array of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        features = np.zeros((len(data), 1))\n",
    "\n",
    "        # Workspace 4.4\n",
    "        # TODO: Transform reviews into the feature (number of positive words)  \n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4505608a30af5dd68819f5d0abb18a7",
     "grade": false,
     "grade_id": "q45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see now how well the classifier would perform with these features that we just engineered. \n",
    "\n",
    "- 4.5 **[3 points]** Define `second_featurizer` to combine:`num_of_sentences`, `num_of_negative_words`, `num_of_positive_words`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bd2b682bcc533f051453765f98855cc",
     "grade": true,
     "grade_id": "a45",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 4.5\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = second_featurizer.fit_transform(imdb.X_train)\n",
    "X_test = second_featurizer.transform(imdb.X_test)\n",
    "print(X_train.shape, X_test.shape)\n",
    "# Train the classifier and find the accuracy on the test dataset\n",
    "lr = SGDClassifier(loss='log', penalty='l2', alpha=0.001, max_iter=2000, shuffle=True, verbose=0, random_state=42)\n",
    "lr.fit(X_train, imdb.y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, imdb.y_test)\n",
    "print(\"Accuracy on testing set: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b690b16cf4701b9d321911443261e35e",
     "grade": false,
     "grade_id": "q46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That's an improvement! Let's try now to include `n-grams` to the list of features.\n",
    "\n",
    "`N-grams` are all combinations of n-length tokens in a given text. This can be at word level (`word n-grams`) or character level (`char n-grams`) \n",
    "\n",
    "For example, if you have the text: `\"The movie was great\"` , then for this text:\n",
    "\n",
    "- `unigrams` (`n=1`) at the word level would be `[\"The\", \"movie\", \"was\", \"great\"]` and \n",
    "- `bigrams` (`n=2`) at the word level would be `[\"The movie\", \"movie was\", \"was great\"]`\n",
    "\n",
    "and you can generalize this to `n-grams`.\n",
    "\n",
    "- 4.6 **[3 points]** Define `third_featurizer` to add include an N-gram extractor at the word level.\n",
    "\n",
    "HINT: use the scikit-learn transformer [`CountVectorizer`](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "793ec8f8073a62d83afeb94272444609",
     "grade": true,
     "grade_id": "a46",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "third_featurizer = FeatureUnion([\n",
    "    # Workspace 4.6\n",
    "    # TODO: Add the three feature transformers from 4.5 and a fourth one to extract word n-grams of range 1 to 3\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    #END\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see if adding n-grams helped boost the classifier performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = third_featurizer.fit_transform(imdb.X_train)\n",
    "X_test = third_featurizer.transform(imdb.X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and find the accuracy on the test dataset\n",
    "lr = SGDClassifier(loss='log', penalty='l2', alpha=0.001, max_iter=2000, shuffle=True, verbose=0, random_state=42)\n",
    "lr.fit(X_train, imdb.y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, imdb.y_test)\n",
    "print(\"Accuracy on testing set: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b843660c35862d9bdf477327d7a7a466",
     "grade": false,
     "grade_id": "q47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "N-grams definitely helped enhance the classifier performance!\n",
    "\n",
    "- 4.7 **(Bonus)** Try to a achieve better accuracy on IMDB dataset. You're restricted to using SGDClassifier, but you're free to tweak its parameters. You'll get 1 bonus point for each 1% accuracy above 82% that you achieve (rounded-up)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0715e07dbab22b28b159d5f876ebd4",
     "grade": true,
     "grade_id": "a47",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 4.7\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
