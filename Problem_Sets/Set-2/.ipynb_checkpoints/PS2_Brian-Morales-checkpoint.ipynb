{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Decision Trees, Model Selection, and Ensemble Methods\n",
    "## CSCI 5622 - Fall 2022\n",
    "***\n",
    "**Name**: Brian Morales \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11:59PM on October 10**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas with the name format `PS2_Brian-Morales.ipynb`. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to homework 1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "294a69c5d0f0d0badb468c04285acd74",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a decision tree classifier from scratch. Of course, we provide some initial classes\n",
    "that you'll be editing. Since last two problems will use the scikit-learn's DecisionTreeClassifier, your solution\n",
    "does not have to be efficient as long as it passes the sanity checks in a reasonable time (typically less than ~1min).\n",
    "\n",
    "We will run a small comparison between our implementation and Scikit's in Problem 2 to make sure we didn't miss anything.\n",
    "\n",
    "The third part will introduce k-fold cross validation to find out how deep is the best decision tree classifier. The last problem\n",
    "requires a _weak learner_, so we'll use a decision tree that yields lower performance. But with _Ensemble Methods_,\n",
    "we will be able to improve the performance by aggregating predictions from multiple weak learners.\n",
    "For the ensemble methods, we'll explore bagging, Random Forest, and boosting (AdaBoost).\n",
    "\n",
    "Any Machine Learning interview will almost certainly have a question or two about decision trees and how they're trained.\n",
    "So understanding the code and trying to implement everything on your own will be the best way to prepare for such interviews.\n",
    "\n",
    "Also remember, if your code is correct then the sanity checks should pass without any major issue.\n",
    "But if the sanity checks pass that does not necessarily imply your code is 100% correct.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b665692ac38f1b9fa4c6f3112b828d5",
     "grade": false,
     "grade_id": "imports_p1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tests\n",
    "import data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "776f8265e4a8630385ee2d3e9af6d654",
     "grade": false,
     "grade_id": "datatable",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1 - Decision Trees [30 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will\n",
    "use the implementation from scikit-learn.\n",
    "\n",
    "We'll be testing our implementation on the same dataset we used for Naive Bayes.\n",
    "\n",
    "|Age|Salary|Colorado Resident| Has Siblings | College degree|\n",
    "|:------:|:-----------:| :----------:| :----------:|--:|\n",
    "| 37 | 44,000 | Yes | No  | Yes|\n",
    "| 61 | 52,000 | Yes | No  | No |\n",
    "| 23 | 44,000 | No  | No  | Yes|\n",
    "| 39 | 38,000 | No  | Yes | Yes|\n",
    "| 48 | 49,000 | No  | No  | Yes|\n",
    "| 57 | 92,000 | No  | Yes | No |\n",
    "| 38 | 41,000 | No  | Yes | Yes|\n",
    "| 27 | 35,000 | Yes | No  | No |\n",
    "| 23 | 26,000 | Yes | No  | No |\n",
    "| 38 | 45,000 | No  | No  | No |\n",
    "| 32 | 50,000 | No  | No  | Yes|\n",
    "| 25 | 52,000 | Yes | No  | Yes|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5422b36081e4897de4a7e2b898124e5",
     "grade": false,
     "grade_id": "p1_data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([\n",
    "    [37, 44000, 1, 0],\n",
    "    [61, 52000, 1, 0],\n",
    "    [23, 44000, 0, 0],\n",
    "    [39, 38000, 0, 1],\n",
    "    [48, 49000, 0, 0],\n",
    "    [57, 92000, 0, 1],\n",
    "    [38, 41000, 0, 1],\n",
    "    [27, 35000, 1, 0],\n",
    "    [23, 26000, 1, 0],\n",
    "    [38, 45000, 0, 0],\n",
    "    [32, 50000, 0, 0],\n",
    "    [25, 52000, 1, 0]\n",
    "])\n",
    "labels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46d6cd24f0ff95644b497fc06917e821",
     "grade": false,
     "grade_id": "q11_12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each leaf node (terminal node) in a decision tree has a label value assigned to it. The same label will be assigned\n",
    "to all samples that reach the leaf node.\n",
    "- 1.1 [2 pts] What is the best accuracy for a baseline classifier that predicts one label for all rows on the dataset above?\n",
    "which label should it predict?\n",
    "- 1.2 [3 pts] Complete `compute_label` to return the label that should be assigned to the leaf node based on training labels in `y`.\n",
    "\n",
    "If more than one label are possible, choose the one with the lowest value (e.g, if both `0` and `1` are possible,\n",
    "choose `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ece7cf4df488ac69fd5a2ae22baadcb5",
     "grade": true,
     "grade_id": "a11",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% Write-up for 1.1 <br>\n",
    "%BEGIN\n",
    "\n",
    "The best accuracy for a baseline classifier that predicts one label for all rows is $58.3\\%$. `1` is the most fequent label totallying $7$ out of the $12$ label samples. Therefore its should predict label `1`. \n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88fc6910e475902184ca6f14b63253b7",
     "grade": true,
     "grade_id": "a12",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Base class for LeafNode and ParentNode\"\"\"\n",
    "    left_child = None\n",
    "    right_child = None\n",
    "    def feature_importance(self, importance_dict):\n",
    "        return importance_dict\n",
    "\n",
    "class LeafNode(Node):\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        :param y: 1-d array containing labels, of shape (num_points,)\n",
    "        \"\"\"\n",
    "        self.label = self.compute_label(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_label(y):\n",
    "        \"\"\"\n",
    "        return the label that yields best performance if predicted of all instances in y\n",
    "        :param y:  1-d array containing labels\n",
    "        :return: single label, integer\n",
    "        \"\"\"\n",
    "        node_label = None\n",
    "        #Workspace 1.2\n",
    "        #TODO: Return the label that should be assigned to the leaf node\n",
    "        #In case of multiple possible labels, choose the one with the lowest value\n",
    "        #Make no assumptions about the number of class labels\n",
    "        #BEGIN \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        tie = np.argwhere(counts == np.amax(counts)).shape \n",
    "        if tie != (1,1):\n",
    "            # if a tie, return the lowest value of the tie \n",
    "            return min(values[counts == counts.max()])\n",
    "        node_label = values[counts == counts.max()]\n",
    "        #END\n",
    "        return node_label\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        return the label for one obervation x\n",
    "        :param x: one sample, of shape (num_features)\n",
    "        :return: label, integer\n",
    "        \"\"\"\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.2: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_leaf(LeafNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0cfdf2ce84fa7ad34134b6a7f56b4be",
     "grade": false,
     "grade_id": "q13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The tree also contains _parent nodes_. They can either be parents of: leaf nodes, parent nodes, or a combination of the two.\n",
    "Each parent node has a left and a right child. A parent node is used when we can reduce the impurity of the labels by splitting\n",
    "the training instances based on a certain threshold.\n",
    "\n",
    "First, we'll need to choose an impurity measure. For classification,\n",
    "there are two mainstream measures: _gini index_ and _entropy_. We'll be using the former for our implementation.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Gini}(y) = 1 - \\sum_{c}  (p_c)^2 \\text{  and  Entropy}(y) = -\\sum_{c}  p_c . \\log p_c ,\n",
    "\\end{align}\n",
    "\n",
    "where $p_c$ is the probability of occurrence (ratio)  of class $c$ among the labels in $y$\n",
    "\n",
    "- 1.3 [3 pts] Complete the function `gini` that returns the gini index of labels in `y`.\n",
    "\n",
    "_Hint: Make sure you handle multi-class labels\n",
    "(not just binary)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8eaef16d6f6787c42a214a242f4a2876",
     "grade": true,
     "grade_id": "a13",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, entropy measure of the labels\n",
    "    \"\"\"\n",
    "    gini_index = 0\n",
    "    # Workspace 1.3\n",
    "    #TODO: Compute the gini index of the labels\n",
    "    #BEGIN \n",
    "    values, occ = np.unique(y ,return_counts=True)\n",
    "    _, index = np.unique(values, return_index=True)\n",
    "    p_c_sum = sum((occ[index]/len(y))**2)\n",
    "    gini_index = 1 - p_c_sum\n",
    "    #END\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.3: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_gini(gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c1f72fb399ad136eeae14a21d5ba616",
     "grade": false,
     "grade_id": "q14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we're at a parent node, we decide to partition our label instances in $S$ to two parts indexed by $P_1$ and $P_2$,\n",
    "and we want to compute how much this split reduces the impurity.\n",
    "\n",
    "Using the impurity measure $\\mathcal{M}$, this impurity reduction is computed as follows:\n",
    "\\begin{align}\n",
    "\\text{Reduction}(S, {P_1, P_2}) = \\mathcal{M}(S) - \\big[\n",
    "    \\frac{|P_1|}{|S|} .\\mathcal{M}(S[P_1]) + \\frac{|P_2|}{|S|}.\\mathcal{M}(S[P_2])\n",
    "    \\big],\n",
    "\\end{align}\n",
    "\n",
    "where $|A|$ denotes the size of the set $A$.\n",
    "\n",
    "The main questions will be based on the entropy measure, in which case the `Reduction` is also called _information gain_\n",
    "(reducing the entropy implies that the partitioning decision variable and the labels have a higher mutual information).\n",
    "\n",
    "-  1.4 [3 pts] Complete the `impurity_reduction` function to return the impurity reduction of the split using the provided measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8d9a87a3208587bd0d8b352a094346e",
     "grade": true,
     "grade_id": "a14",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def impurity_reduction(y, left_indices, right_indices, impurity_measure=gini):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param left_indices: the indices of the elements of y that belong to the left child\n",
    "    :param right_indices: the indices of the elements of y that belong to the right child\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure, defaults to gini\n",
    "    :return: impurity reduction of the split\n",
    "    \"\"\"\n",
    "    impurity_reduce = 0\n",
    "    # Workspace 1.4\n",
    "    #BEGIN \n",
    "    gini_s = impurity_measure(y)\n",
    "    size_y = len(y)\n",
    "    p_1    = len(left_indices)\n",
    "    p_2    = len(right_indices)\n",
    "    imprty_s_p1 = impurity_measure(y[left_indices])\n",
    "    imprty_s_p2 = impurity_measure(y[right_indices])\n",
    "    \n",
    "    impurity_reduce = gini_s - [(p_1 / size_y)*imprty_s_p1 + (p_2 / size_y)*imprty_s_p2]\n",
    "    \n",
    "    #END\n",
    "    return impurity_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.4: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_information_gain(impurity_reduction, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56760878d20b0568d67c7aeb77471248",
     "grade": false,
     "grade_id": "q15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll use `best_partition` to look up for the feature and threshold that yields the partition with the best impurity reduction.\n",
    "\n",
    "For each feature:\n",
    " - Compute all possible thresholds (use `split_values`)\n",
    " - For each threshold:\n",
    "    - Split to `(left_indices, right_indices)` based on the threshold\n",
    "    - Compute the impurity reduction of the split\n",
    "\n",
    "The function then returns the feature and the threshold that yield the best impurity reduction (and the reduction value)\n",
    "\n",
    " - 1.5 [5 pts] Complete `best_partition`.\n",
    " \n",
    " _Hint: `split_values` is provided as a helper function. It takes the feature column and returns\n",
    "the set of thresholds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d9787709bde545d2117cf2f2c116417",
     "grade": true,
     "grade_id": "a15",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_values(feature_values):\n",
    "    \"\"\"\n",
    "    Helper function to return the split values. if feature consists of the values f1 < f2 < f3 then\n",
    "    this returns [(f2 + f1)/2, (f3 + f2)/2]\n",
    "    :param feature_values: 1-d array of shape (num_points)\n",
    "    :return: array of shape (max(m-1, 1),) where m is the number of unique values in feature_values\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(feature_values)\n",
    "    if unique_values.shape[0] == 1:\n",
    "        return unique_values\n",
    "    return (unique_values[1:] + unique_values[:-1]) / 2\n",
    "\n",
    "\n",
    "def best_partition(X, y, impurity_measure=gini):\n",
    "    \"\"\"\n",
    "    :param X: features array, shape (num_samples, num_features)\n",
    "    :param y: labels of instances in X, shape (num_samples)\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on the different features.\n",
    "    \"\"\"\n",
    "\n",
    "    best_feature, best_threshold, best_reduction = 0, 0, -np.inf\n",
    "\n",
    "    #Workspace 1.5\n",
    "    #TODO: Complete the function as detailed in the question and return description\n",
    "    #BEGIN \n",
    "    list_imp_red = []\n",
    "    list_feature = []\n",
    "    list_threshold = []\n",
    "    for col in range(X.shape[1]): \n",
    "        # retrieve threshold list for feature column\n",
    "        threshold_list = split_values(X[:, col])\n",
    "        for i in range(len(threshold_list)):\n",
    "            #split based on current threshold\n",
    "            left = np.where(X[:, col] < threshold_list[i])[0]\n",
    "            right = np.where(X[:, col] >= threshold_list[i])[0]\n",
    "            \n",
    "            # append all features, thresholds, and impurity reductions in order\n",
    "            list_feature.append(col)\n",
    "            list_threshold.append(threshold_list[i])\n",
    "            list_imp_red.append(impurity_reduction(y, left, right, impurity_measure)[0])\n",
    "\n",
    "    best_feature   = list_feature[np.argmax(list_imp_red)] \n",
    "    best_threshold = list_threshold[np.argmax(list_imp_red)] \n",
    "    best_reduction = list_imp_red[np.argmax(list_imp_red)]\n",
    "    #END\n",
    "    return best_feature, best_threshold, best_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.5: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_best_partition(best_partition, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62c52953bd532bb6a14a4ed3b1dc2de6",
     "grade": false,
     "grade_id": "parent_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide the implementation of the parent node below. Note that the `left_child` will take instance for which\n",
    "`feature_id` value is < `feature_threshold`. We should construct our decision tree as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ee7501059904c4a686b5236adde493a",
     "grade": true,
     "grade_id": "a25a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ParentNode:\n",
    "\n",
    "    def __init__(self, feature_id, feature_threshold, left_child: Node, right_child: Node, weighted_impurity=0):\n",
    "        \"\"\"\n",
    "        Initialize a parent node.\n",
    "        :param feature_id: the feature index on which the splitting will be done\n",
    "        :param feature_threshold: the feature threshold. Left child takes item with features[features_id] < threshold\n",
    "        :param left_child: left child node\n",
    "        :param right_child: right child node\n",
    "        :param weighted_impurity: weighted impurity reduction, optional (used for the bonus question)\n",
    "        \"\"\"\n",
    "        self.feature_id = feature_id\n",
    "        self.threshold = feature_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.weighted_impurity = weighted_impurity\n",
    "\n",
    "    def feature_importance(self, importance_dict):\n",
    "        \"\"\"\n",
    "        :param importance_dict: dictionary, keys are features indices and value are feature importances\n",
    "        :return: updated feature importrances dictionary\n",
    "        \"\"\"\n",
    "        #Workspace 2.5.a\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return importance_dict\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label of row x. If we're a leaf node, return the value of the leaf. Otherwise, call predict\n",
    "        of the left/right child (depending on x[feature_index).\n",
    "        This will be called by DecisionTree.predict\n",
    "        :param x: 1-d array of shape (num_features)\n",
    "        :return: integer, the label for x\n",
    "        \"\"\"\n",
    "        if x[self.feature_id] < self.threshold:\n",
    "            label = self.left_child.predict(x)\n",
    "        else:\n",
    "            label = self.right_child.predict(x)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a136ad8e62d303bc69f292b2216f102",
     "grade": false,
     "grade_id": "q16_17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we tackle the core of a decision tree. The tree is built in a recursive way. The recursion in `DecisionTree.build` works as follows:\n",
    "- Parameters: `min_samples_split`, `impurity_measure`\n",
    "- Inputs: `features`, `labels`, `depth`\n",
    "- Base case of the recursion, return a leaf node if either:\n",
    "    - `depth` is 0\n",
    "    - `labels` contains less than `min_samples_split` elements\n",
    "    - There is no impurity reduction (reduction<=0 for all splits)\n",
    "- Recursion (there is a split with impurity reduction > 0):\n",
    "    - create the left and right child nodes with `depth - 1`\n",
    "    - return the parent node\n",
    "\n",
    "The left child node will contain instances for which the feature with index `best_feature` is strictly lower than\n",
    "`best_threshold` of the partition. The right child takes the remaining instances.\n",
    "\n",
    "- 1.6 [6 pts] Complete `build` method of `DecisionTree`\n",
    "- 1.7 [2 pts] Complete the `score` method that returns the accuracy on the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c8fb3a06f40ba8a05c09793bf9d957b",
     "grade": true,
     "grade_id": "a16_17",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=-1, min_samples_split=2, impurity_measure=gini):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        :param max_depth: maximum depth of the tree\n",
    "        :param min_samples_split: minimum number of samples required for a split\n",
    "        :param impurity_measure: impurity measure function to use for best_partition, default to entropy\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.root = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def build(self, X, y, depth) -> Node:\n",
    "        \"\"\"\n",
    "        Recursive method used to build the decision tree nodes\n",
    "        :param X: data that are used to build the tree, of shape (num_samples, num_features)\n",
    "        :param y: labels of the samples in features, of shape (num_samples)\n",
    "        :param depth: depth of the tree to create\n",
    "        :return: the root node of the tree\n",
    "        \"\"\"\n",
    "        # Workspace 1.6\n",
    "        #BEGIN\n",
    "        \n",
    "        # best partition\n",
    "        best_feature, best_threshold, best_reduction = best_partition(X, y, self.impurity_measure)\n",
    "        #print('depth: ', depth)\n",
    "        if depth == 0 or (len(y) < self.min_samples_split) or best_reduction <= 0:\n",
    "            return LeafNode(y)\n",
    "        else:\n",
    "            # indices for left child\n",
    "            left_indices = np.where(X[:,best_feature] < best_threshold)[0]\n",
    "\n",
    "            # remove left child instances and keep remaining instances in right child\n",
    "            right_indices = np.where(X[:, best_feature] > best_threshold)[0]\n",
    "\n",
    "            # get the values at the index\n",
    "            left_values  = X[left_indices, :]\n",
    "            right_values = X[right_indices, :]\n",
    "            \n",
    "            # y values corresponding to node\n",
    "            y_left = y[left_indices]\n",
    "            y_right = y[right_indices]\n",
    "\n",
    "            return ParentNode(best_feature, best_threshold, \n",
    "                                self.build(left_values, y_left, depth - 1), \n",
    "                                self.build(right_values, y_right, depth -1),\n",
    "                                weighted_impurity=0)\n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: Training samples\n",
    "        :param y: training labels\n",
    "        :return: trained classifier\n",
    "        \"\"\"\n",
    "        self.num_features = X.shape[1]\n",
    "        self.root = self.build(X, y, self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def compute_importance(self, features_names=None):\n",
    "        \"\"\"\n",
    "        Compute the normalized feature importances\n",
    "        :param features_names: Name of features to use, defaults to integers\n",
    "        :return: Dictionary with feature_name: feature_importance\n",
    "        \"\"\"\n",
    "        if features_names is None:\n",
    "            features_names = [\"feat_%i\" % i for i in range(self.num_features)]\n",
    "        feats_importances = {i:0.0 for i in range(self.num_features)} # to include\n",
    "        # Workspace 2.5.b\n",
    "        # ToDo: Call the root's feature and importance and scale values in feats_importance to sum to 1\n",
    "        total_importances = 1\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return {features_names[k] :v for k,v in feats_importances.items() if v>0}\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Loops through rows of X and predicts the labels one row at a time\n",
    "        \"\"\"\n",
    "        y_hat = np.zeros((X.shape[0],), int)\n",
    "        for i in range(X.shape[0]):\n",
    "            y_hat[i] = self.root.predict(X[i])\n",
    "        return y_hat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN \n",
    "        y_hat = self.predict(X)\n",
    "        accuracy =  len(y[y_hat == y])/len(y)\n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.6: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_tree_build(DecisionTree, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd5a3c587c8ec35f8f9c702f84d29933",
     "grade": false,
     "grade_id": "q18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 1.8 [2 pts] We want to compare our `DecisionTree(max_depth=3, min_samples_split=2` to our NaiveBayes.\n",
    "What's the accuracy we achieve on the training data using the tree? ( we train and evaluate using `(features, labels)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4015467f2f89548d668cbd7393e9a4cf",
     "grade": true,
     "grade_id": "a18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Workspace 1.8\n",
    "#BEGIN \n",
    "dt = DecisionTree(max_depth=3, min_samples_split=2).fit(features, labels)\n",
    "dt.score(features,labels)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38756b9794ee2734fba840eeca4da2ab",
     "grade": false,
     "grade_id": "q19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.9 [2 pts] Using `min_samples_split=2`, what is the minimum depth so that our `DecisionTree` fits perfectly our\n",
    "training data `(labels, features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a67bc7261215441b222be45948125d92",
     "grade": true,
     "grade_id": "a19",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  0.75\n",
      "2 :  0.9166666666666666\n",
      "3 :  0.9166666666666666\n",
      "4 :  0.9166666666666666\n",
      "5 :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Workspace 1.9\n",
    "# To show that the minimum required depth is n, you can provide the accuracy for depth = (n-1) and depth = n\n",
    "#BEGIN \n",
    "for n in range(1,6):\n",
    "    dt = DecisionTree(max_depth=n, min_samples_split=2).fit(features, labels)\n",
    "    print(n, ': ', dt.score(features,labels))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum depth to achieve a perfect score is $5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ac784f19dbd195c8c502ecf11891fd7",
     "grade": false,
     "grade_id": "q110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide an example below to display the structure of a decision tree. Look at print_tree() in tests.\\_\\_init\\_\\_.py to understand how this visualization is working.\n",
    "- 1.10 (2pts) Edit it to show the tree for the required minimum depth found in 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d30ddb7d50cc5b1bed552b1e0dd66de7",
     "grade": true,
     "grade_id": "a110",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ┌│label: 0\n",
      "       ┌|salary  │┘\n",
      "       │|36500.00│┐\n",
      "       │          │       ┌│label: 1\n",
      "       │          └|age  │┘\n",
      "       │           |37.50│┐\n",
      "       │                  └│label: 1\n",
      "|age  │┘\n",
      "|52.50│┐\n",
      "       └│label: 0\n",
      "                  ┌│label: 0\n",
      "       ┌|salary  │┘\n",
      "       │|36500.00│┐\n",
      "       │          │       ┌│label: 1\n",
      "       │          └|age  │┘\n",
      "       │           |37.50│┐\n",
      "       │                  │                  ┌│label: 1\n",
      "       │                  │       ┌|salary  │┘\n",
      "       │                  │       │|43000.00│┐\n",
      "       │                  │       │          └│label: 0\n",
      "       │                  └|age  │┘\n",
      "       │                   |38.50│┐\n",
      "       │                          └│label: 1\n",
      "|age  │┘\n",
      "|52.50│┐\n",
      "       └│label: 0\n"
     ]
    }
   ],
   "source": [
    "#BEGIN \n",
    "# change here\n",
    "tree = DecisionTree(max_depth=3, min_samples_split=2).fit(features, labels)\n",
    "tests.print_tree(tree, [\"age\", \"salary\", \"resident\", \"siblings\"])\n",
    "#END#BEGIN \n",
    "tree = DecisionTree(max_depth=5, min_samples_split=2).fit(features, labels)\n",
    "tests.print_tree(tree, [\"age\", \"salary\", \"resident\", \"siblings\"])\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c314d39fc2cc0d572aefaea63e78c669",
     "grade": false,
     "grade_id": "q21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 2: DecisionTree vs DecisionTreeClassifier [6 points]\n",
    "\n",
    "We've just showed that our decision tree is better than the naive NaiveBayes! Let see how it compares to scikit's\n",
    "DecisionTreeClassifier.\n",
    "\n",
    "First, we'll need a fancier dataset. We are going to predict the level of usage of a bike sharing system in Washington, DC using the decision trees.\n",
    "\n",
    "We start by loading preprocessed data that we'll use. Since the original Bike Sharing\n",
    " [dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)\n",
    " is for regression, we have to transform `BikeSharing.y_train` and `BikeSharing.y_test` to discrete values reflecting the level of usage.\n",
    "We have included this dataset with the homework -- you can find it in the data directory.\n",
    "\n",
    "|Bike Rentals| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $2000|0|\n",
    "|2000$\\leq P < $ 4000| 1 |\n",
    "|4000$ \\leq P < $ 6000| 2 |\n",
    "|6000$ \\leq P $ | 3 |\n",
    "\n",
    "- 2.1 [3 pts] Start by transforming `y_train` and `y_test` of `bike_sharing` to discrete values using the provided ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17d7a5fce728e39b29c0cb3591cf8f7f",
     "grade": true,
     "grade_id": "a21",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3.] (584, 12)\n",
      "[0. 1. 2. 3.] (147, 12)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "bike_sharing = data.BikeSharing()\n",
    "#Workspace 2.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "#BEGIN\n",
    "# training set\n",
    "bike_sharing.y_train[bike_sharing.y_train < 2000] = 0\n",
    "bike_sharing.y_train[(2000 <= bike_sharing.y_train) & ( bike_sharing.y_train < 4000)] = 1\n",
    "bike_sharing.y_train[(4000 <= bike_sharing.y_train) & (bike_sharing.y_train < 6000)] = 2\n",
    "bike_sharing.y_train[6000 <= bike_sharing.y_train] = 3\n",
    "# testing set\n",
    "bike_sharing.y_test[bike_sharing.y_test < 2000] = 0\n",
    "bike_sharing.y_test[(2000 <= bike_sharing.y_test) & (bike_sharing.y_test < 4000)] = 1\n",
    "bike_sharing.y_test[(4000 <= bike_sharing.y_test) & (bike_sharing.y_test < 6000)] = 2\n",
    "bike_sharing.y_test[6000 <= bike_sharing.y_test] = 3\n",
    "\n",
    "#END\n",
    "\n",
    "print(np.unique(bike_sharing.y_train), bike_sharing.X_train.shape)\n",
    "print(np.unique(bike_sharing.y_test), bike_sharing.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "648343ab4012a08c34fd86a7eeadaf2b",
     "grade": false,
     "grade_id": "q22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 2.2 [3 pts] Compare our `DecisionTree` and scikit's `DecisionTreeClassifier` on the bike sharing dataset by reporting the accuracies on the test data.\n",
    "\n",
    " [scikit's `DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "uses Gini Index by default and shuffles the features before each split. Refer to the documentation for more information about how to change the impurity measure if you are curious.\n",
    "\n",
    "Use `max_depth = 5, min_samples_split=2, random_state=11` for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c8023ee992a7a22b7c1c8a0a6c5d6e5",
     "grade": true,
     "grade_id": "a22a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Decision Tree Accucacy:  0.7074829931972789\n",
      "Sklearn Accuracy:  0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "# Workspace 2.2.a\n",
    "#BEGIN \n",
    "X_bike = bike_sharing.X_train\n",
    "y_bike = bike_sharing.y_train\n",
    "x_test = bike_sharing.X_test\n",
    "y_test  = bike_sharing.y_test\n",
    "my_DT = DecisionTree(max_depth=5, min_samples_split=2).fit(X_bike, y_bike)\n",
    "print(\"My Decision Tree Accucacy: \", my_DT.score(x_test, y_test))\n",
    "\n",
    "sklearn_DT = DecisionTreeClassifier(max_depth=5, min_samples_split = 2, random_state = 11).fit(X_bike, y_bike)\n",
    "print('Sklearn Accuracy: ', sklearn_DT.score(x_test, y_test))\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93d9f093912aeaef77d0e03e3f5c0d83",
     "grade": true,
     "grade_id": "a22b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "%BEGIN\n",
    "\n",
    "The reported accuracy for both classifiers are the same. My Decision Tree does as well as scikit learns decision tree. \n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6834fe9c195045f4b03a06cb2c4b78c1",
     "grade": false,
     "grade_id": "q23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bonus questions\n",
    "We've implemented `DecisionTree` to handle different measures of impurity. We want now to compare our implementation\n",
    "to the standard `DecisionTreeClassifier` using Gini index.\n",
    "- **(Bonus)** 2.3  [2 pts] Complete `entropy` function\n",
    "_hint: for the log function, use `np.log` and the convention `0 * log(0) = 0`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecfb6ce406d6bf8be72a260413be5bc1",
     "grade": true,
     "grade_id": "a23",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, gini index the labels\n",
    "    \"\"\"\n",
    "    entropy_value = 0\n",
    "    # Workspace 2.3\n",
    "    #TODO: Compute the gini index of the labels in y\n",
    "    #BEGIN \n",
    "    values, occ = np.unique(y ,return_counts=True)\n",
    "    _, index = np.unique(values, return_index=True)\n",
    "    pc = (occ[index]/len(y))\n",
    "    log_pc = np.log(pc) \n",
    "    entropy_value = -sum(pc * log_pc)\n",
    "    #END\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29019ac7127cbcc8a348aa710f88bb7b",
     "grade": false,
     "grade_id": "q24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **(Bonus)** 2.4 [2 pts] Perform the same comparison as in 2.2 with entropy but without setting the random state.\n",
    "How do you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d9411484d9bac971d2b86f534f236f9",
     "grade": true,
     "grade_id": "a24a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Decision Tree Accucacy (Impurity Measure: Entropy):  0.7142857142857143\n",
      "Sklearn Accuracy (Impurity Measure: Entropy):  0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2) # to fix the randomness in DecisionTreeClassifier\n",
    "# Workspace 2.4.a\n",
    "#BEGIN \n",
    "my_DT = DecisionTree(max_depth=5, min_samples_split=2, impurity_measure=entropy).fit(X_bike, y_bike)\n",
    "print(\"My Decision Tree Accucacy (Impurity Measure: Entropy): \", my_DT.score(x_test, y_test))\n",
    "\n",
    "sklearn_DT = DecisionTreeClassifier(max_depth=5, min_samples_split = 2, criterion='entropy').fit(X_bike, y_bike)\n",
    "print('Sklearn Accuracy (Impurity Measure: Entropy): ', sklearn_DT.score(x_test, y_test))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c6bd07d28bca6cede8aea31ecf1872d",
     "grade": true,
     "grade_id": "a24b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Write-up for 2.4.b\n",
    "%BEGIN\n",
    "\n",
    "My `DecisionTree` classifier performs better than `scikit learns` decision tree classifier. According to `scikit learns` website, entropy is calculated with consideration of 'the class frequencies of the training data points that reached a given leaf  $m$ as their probability.' If I understand correctly, we calculate the given `y` corresponding to each lead. \n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d042609e13d7e090dbfbc1a3afdea36",
     "grade": false,
     "grade_id": "q25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Bonus)**\n",
    "\n",
    "Now we can be a bit more ambitious and compute the importance of each feature in our decision tree. The importance of feature $f$\n",
    "is the sum of the weighted impurity reduction of parent nodes that are split based on the feature $f$.\n",
    "\n",
    "The weighted impurity reduction of $node_i$ is the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{N_{\\text{node}_i}}{N_\\text{total}} \\times \\text{impurity reduction}({\\text{node}_i}),\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the total number of training samples, and $N_{\\text{node}_i}$ is the number of training samples that at $node_i$.\n",
    "\n",
    "Since we scale the feature importances in `DecisionTree` to sum to 1, we don't have to divide by $N_\\text{total}$\n",
    "and we can simply use:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{weighted impurity}(\\text{node}_i) = N_{\\text{node}_i} \\times \\text{impurity reduction}({\\text{node}_i}),\n",
    "\\end{align}\n",
    "\n",
    "Practically, we use a dictionary `feats_importances` that maps feature indices to their importances.\n",
    "- Start with `feats_importance[f]=0` for all `f`\n",
    "- Start the recursion from the root node:\n",
    "    - Current node is split based on feature `i`\n",
    "    - add weighted impurity reduction to `feature_importance[i]`\n",
    "    - ask right and left child to do the same\n",
    "- Scale the values in `feats_importance` to sum to 1   \n",
    "- return `feats_importance`\n",
    "\n",
    "You can provide `weighted_impurity` directly when initializing the parent nodes in `DecisionTree.build`.\n",
    "\n",
    "- **(Bonus)** 2.5 [4 pts] Complete `ParentNode`'s `feature_importance`, `DecisionTree`'s `compute_importance`, and \n",
    "compare our implementation to that of scikits on bike sharing data.\n",
    "\n",
    "Use `random_state=0, splitter=\"best\"` for scikit and `max_depth=3`, `min_samples_split=2`, gini index for both.\n",
    "Note that scikit's DecisonTreeClassifier always uses Gini for the feature importance computation (even if `criterion` is set to Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16b9cafda3c1f57441eed63955da1ea",
     "grade": true,
     "grade_id": "a25c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 2.5\n",
    "# Compare feature importances of DecisionTree(gini) to DecisionTreeClassifier\n",
    "# Exclude features with 0 importance from both\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea9daf8bcc8578d8dd9a2590bf4b4f0",
     "grade": false,
     "grade_id": "q31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 3 - Model Selection via Cross Validation [16 points]\n",
    "***\n",
    "In this problem, we will be working with scikit-learn `DecisionTreeClassifier`. We want to figure out the best `max_depth`\n",
    " for our dataset.\n",
    "\n",
    "In the bike sharing dataset, we only have a training set and a test set. The question then is how do we perform the model\n",
    " selection seen in Problem Set 1?\n",
    "\n",
    "One way to do so is via **the cross validation set approach** which basically means setting aside a portion of\n",
    "our training data to use as a validation set. The goal is to use the validation set to find the best hyperparameters\n",
    "for our model (`max_depth` in the case of decision trees).\n",
    "\n",
    "- 3.1 [3 points] complete the `cross_validate` function to train the classifier on the training set and\n",
    "return the accuracy on the validation set based on provided indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb9b6096d9f2ecbcd8d9c331ea8467ee",
     "grade": true,
     "grade_id": "a31",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate(classifier, X, y, train_indices, valid_indices):\n",
    "    \"\"\"\n",
    "    Train classifier on training set and validate on the validation set\n",
    "    :param classifier: the classifier to use\n",
    "    :param X: all data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :param train_indices:  indices to be used for training the model\n",
    "    :param valid_indices:  indices to be used for validating the model\n",
    "    :return: he accuracy of the classifier on the validation set\n",
    "    \"\"\"\n",
    "    valid_accuracy = 0\n",
    "    #Workspace 3.1\n",
    "    #TODO: train and validate the model based on provided indices\n",
    "    #Hint: use score method of the classifier\n",
    "    #BEGIN \n",
    "    \n",
    "    x_train = X[ :train_indices, :  ]\n",
    "    y_train = y[:train_indices]\n",
    "    x_valid = X[-valid_indices :, :]\n",
    "    y_valid = y[-valid_indices:]\n",
    "    model = classifier.fit(x_train, y_train)\n",
    "    valid_accuracy = model.score(x_valid, y_valid)\n",
    "    #END\n",
    "    \n",
    "    return valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cea5adf3217a3118305ca599da8ad723",
     "grade": false,
     "grade_id": "q32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 3.2 [2 points] Report the validation accuracy using the validation set approach for scikit-learn `DecisionTreeClassifier` with `max_depth=3`\n",
    " when using the last 100 training points as a validation set and the rest as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736c9be3f2b43025375649f8426c5eb6",
     "grade": true,
     "grade_id": "a32",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Workspace 3.2\n",
    "#TODO: Report the cross validation accuracy using the last 100 training points as validation set\n",
    "#and the rest of the training points as training\n",
    "#BEGIN \n",
    "cross_validate(DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state = 11), \n",
    "               X_bike, y_bike, train_indices=484, valid_indices=100)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d439516863ed619dc20b0f36f39aca16",
     "grade": false,
     "grade_id": "q33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The issue with the validation set approach is that we're reducing the size of our training data,\n",
    " and the lower number of samples implies higher uncertainty.\n",
    "\n",
    "A work-around is to use *k-fold cross validation*.\n",
    "We start by partitioning the training data into k different and equally size partitions.\n",
    "Then for each of the k runs, we keep a different chunk for the validation while using the remaining k-1 for training.\n",
    "We note the validation accuracy during each of the k runs.\n",
    "\n",
    "After each of the k-folds has been used as a validation set, the average of the k recorded accuracies becomes the performance of our model.\n",
    "The k-fold cross validation method gives us a better estimate on how well the model would perform on new unseen data\n",
    " (test set) while allowing it to train on a larger portion of the dataset.\n",
    "- 3.3 [5 points] Complete `k_fold_cv`. Use the helper function `generate_folds` that generates the partition of indices to k different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7788cecdea75335dade85bbc3b12cfbd",
     "grade": true,
     "grade_id": "a33",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_folds(size, k):\n",
    "    \"\"\"\n",
    "    Shuffles and partition range(size) to to k contiguous chunks then generates the train/valid indices for the k-fold\n",
    "    To use as a generator, for an example run:\n",
    "        for train_idx, valid_idx in generate_folds(10,3): print(train_idx, valid_idx)\n",
    "    :param size: size of the range that should be split\n",
    "    :param k: number of folds\n",
    "    :return: iterable of different k splits, each is a tuple (train_indices, valid_indices)\n",
    "             where len(valid_indices)~ size/k\n",
    "    \"\"\"\n",
    "    permutation = np.random.RandomState(seed=42).permutation(size)\n",
    "    split_sizes = [size//k + (i < (size % k)) for i in range(k)] # we split the remainder amongst the first folds\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        # valid indices of i-th split for which start <= σ < start + size_split[i]\n",
    "        # for_valid is True in position where condition is true, False otherwise\n",
    "        for_valid = np.logical_and(start<= permutation, permutation< start + split_sizes[i])\n",
    "        start += split_sizes[i] # update the start of the fold\n",
    "        valid_indices = np.where(for_valid)[0]\n",
    "        # train indices of i-th split for which σ <start or  start + size_split[i] <= σ\n",
    "        # ~bool_array is negation of bool_array\n",
    "        train_indices = np.where(~for_valid)[0]\n",
    "        yield train_indices, valid_indices\n",
    "\n",
    "def k_fold_cv(classifier, k, X, y):\n",
    "    \"\"\"\n",
    "    This function performs k-fold cross validation\n",
    "    :param classifier: a classifier to be used\n",
    "    :param k: number of folds\n",
    "    :param X: all training data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :return: the average accuracy of the classifier in k-runs\n",
    "    \"\"\"\n",
    "    mean_accuracy = 0\n",
    "    #Workspace 3.3\n",
    "    #BEGIN\n",
    "    k_accuracies = []\n",
    "\n",
    "    for train_idx, valid_idx in generate_folds(X.shape[0], k):\n",
    "        k_minus_one_folds = train_idx \n",
    "        curr_kvalid_fold = valid_idx\n",
    "        \n",
    "        x_k_train = X[k_minus_one_folds]\n",
    "        y_k_train = y[k_minus_one_folds]\n",
    "        \n",
    "        x_k_valid = X[curr_kvalid_fold]\n",
    "        y_k_valid = y[curr_kvalid_fold]\n",
    "        \n",
    "        model = classifier.fit(x_k_train, y_k_train)\n",
    "        k_accuracies.append(model.score(x_k_valid, y_k_valid))  \n",
    "    mean_accuracy = np.mean(k_accuracies)\n",
    "    #END\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c36dcc37598efc8e68ce38bf36f93a9f",
     "grade": false,
     "grade_id": "q34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 3.4 [4 points] Consider depths from 1 to 10. Perform hyperparameter search by doing 8-fold cross validation for each depth. What is the best value of `max_depth` and what is the best cross validation accuracy you find over the validation splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa2f1e4417169b3f8ce01f1ddd577f00",
     "grade": true,
     "grade_id": "a34",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy for chosen best max_depth 3: 0.760274\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)  # changing the seed might yield different results\n",
    "best_depth, best_accuracy = -1, 0\n",
    "\n",
    "#Workspace 3.4\n",
    "#TODO: \n",
    "#BEGIN \n",
    "for depth in range(1, 11):\n",
    "    ate_k_folds_acc = k_fold_cv(DecisionTreeClassifier(max_depth=depth, min_samples_split=2), 8, X_bike, y_bike)\n",
    "    if ate_k_folds_acc >=  best_accuracy:\n",
    "        best_accuracy = ate_k_folds_acc\n",
    "        best_depth = depth\n",
    "\n",
    "#END\n",
    "print(\"Cross validation accuracy for chosen best max_depth %d: %f\" % (best_depth, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7b3f1f8db4e540dc42cf99c7e9f0c59",
     "grade": false,
     "grade_id": "q35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 3.5 [2 pts] Train a new model on the entire training set with the best `max_depth` you found above. Report the accuracy of the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80f577dd7355453cd0e1e2adae377d07",
     "grade": true,
     "grade_id": "a35",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the best model on the testing set 0.6802721088435374\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "#Workspace 3.5\n",
    "#BEGIN \n",
    "dt_model = DecisionTreeClassifier(max_depth=best_depth, min_samples_split=2).fit(X_bike, y_bike)\n",
    "test_accuracy = dt_model.score(x_test, y_test)\n",
    "#END\n",
    "print (\"accuracy of the best model on the testing set\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e761e0f37dc0f895c09d10c19bdb0b91",
     "grade": false,
     "grade_id": "p4_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 4  - Decision Tree Ensembles: Bagging and Boosting [48 points]\n",
    "---\n",
    "We've seen that a DecisionTreeClassifier with depth = 3 is far from being the best performing on our bike sharing data.\n",
    "\n",
    "In this problem, we will introduce 3 ensemble methods to _boost_ the performance of this poor and underestimated weak learner.\n",
    "\n",
    "Whenever we need to generate a new instance of our weak learner, we'll have to call `get_weak_leaner`.\n",
    "You can see below that the weak learner achieves lower accuracy compared to the tree from the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33ca93589b8aa21fed36aff8be61a28d",
     "grade": false,
     "grade_id": "weak_learner",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_learner():\n",
    "    \"\"\"Return a new instance of out chosen weak learner\"\"\"\n",
    "    return DecisionTreeClassifier(max_depth=3, min_samples_leaf=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.7722602739726028\n",
      "Accuracy on the test set:     0.6258503401360545\n"
     ]
    }
   ],
   "source": [
    "weak_clf = get_weak_learner().fit(bike_sharing.X_train, bike_sharing.y_train)\n",
    "print(\"Accuracy on the training set:\", weak_clf.score(bike_sharing.X_train,bike_sharing.y_train))\n",
    "print(\"Accuracy on the test set:    \", weak_clf.score(bike_sharing.X_test,bike_sharing.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bbf580a464d72c0c240e27d4a3a804c",
     "grade": false,
     "grade_id": "q41_42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Start by completing the `EnsembleTest` class that we'll use to evaluate different ensemble methods.\n",
    "\n",
    "- 4.1 [5 points] Complete `evaluate_model` to fit the model received as parameter and store the metrics and running time.\n",
    "- 4.2 [4 points] Complete `plot_metric` to show and compare different statistics of each model in a bar chart.\n",
    "\n",
    "You can use [matplotlib](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html) for 4.2. For matplotlib,\n",
    "You'll need to use the first two arguments and `tick_label` to provide the bars labels. Feel free to use seaborn or any other\n",
    "mainstream packages. You can go back to Problem Set 1 and see how we displayed the scatter plots of binary data in the same figure.\n",
    "\n",
    "We have also imported `precision_score` from scikit-learn for you to use in this problem. Make sure to use the `weighted` argument for taking the average when using this. More information is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878f15397ed59ab46f37036ae7a696a7",
     "grade": true,
     "grade_id": "a41_42",
     "locked": false,
     "points": 9,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        initialize EnsembleTest\n",
    "        :param data: dataset containing Training and Test sets\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.execution_time = {} # dictionary with key: model name, value: time taken to fit and score the model\n",
    "        self.metric = {} # dictionary with key: model name, value: accuracy\n",
    "        self.scores = {}# dictionary with key: model name, value: weighted average precision\n",
    "        self.score_name = 'Precision(weighted)'\n",
    "        self.metric_name = 'Mean accuracy'\n",
    "\n",
    "    def evaluate_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model using the training data and save the evaluations metrics on the test set\n",
    "        :param model: classifier to evaluate\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        #Workspace 4.1\n",
    "        #TODO: Fit the model and get the predictions to compute the metric and the score\n",
    "        #BEGIN \n",
    "        model.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "        y_pred = model.predict(self.dataset.X_test)\n",
    "        accuracy = len(self.dataset.y_test[y_pred == self.dataset.y_test])/len(self.dataset.y_test)\n",
    "        precision = precision_score(self.dataset.y_test, y_pred, average='weighted')\n",
    "        \n",
    "        self.metric[name] = accuracy\n",
    "        self.scores[name] = precision\n",
    "        #END\n",
    "        self.execution_time[name] = time() - start\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model': list(self.metric.keys()),\n",
    "            self.score_name: list(self.scores.values()),\n",
    "            self.metric_name: list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"\n",
    "        Plot bar chart, one for each statistic (metric, score, running time)\n",
    "        \"\"\"\n",
    "        #Workspace 4.2\n",
    "        #TODO: plot each metric : time, metric, score\n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        fig.set_figheight(6), fig.set_figwidth(18)\n",
    "        #BEGIN \n",
    "        name = list(self.metric.keys())\n",
    "        metric = list(self.metric.values())\n",
    "        scores = list(self.scores.values())\n",
    "        time = list(self.execution_time.values())\n",
    "        axs[0].bar(name, metric)\n",
    "        axs[0].set_ylabel(\"Accuracy\")\n",
    "        axs[0].set_title('Metric')\n",
    "        axs[1].bar(name, scores)\n",
    "        axs[1].set_ylabel(\"Precision\")\n",
    "        axs[1].set_title('Scores')\n",
    "        axs[2].bar(name, time)\n",
    "        axs[2].set_ylabel(\"Time\")\n",
    "        axs[2].set_title('Execution Time')\n",
    "        #END\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95aefae4f2d3e697a3fda9a8139ffae0",
     "grade": false,
     "grade_id": "q43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 4.3 [2 points] Test `EnsembleTest` using our weak learner returned by `get_weak_learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c4f8112df8cb829430db7165d988c00",
     "grade": true,
     "grade_id": "a43",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model  Precision(weighted)  Mean accuracy  Execution time\n",
      "1  Scikit             0.685739       0.680272        0.002921\n",
      "0      WL             0.639790       0.625850        0.004021\n"
     ]
    }
   ],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(bike_sharing)\n",
    "#Workspace 4.3\n",
    "#TODO: Initialize weak learner and evaluate it using evaluate_model\n",
    "#BEGIN \n",
    "ensemble_handler.evaluate_model(get_weak_learner(), 'WL')\n",
    "ensemble_handler.evaluate_model(DecisionTreeClassifier(max_depth=3, min_samples_split=2), 'Scikit')\n",
    "ensemble_handler.plot_metrics()\n",
    "#END\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b7aee8bb8071199e88a8e1462b4caa7",
     "grade": false,
     "grade_id": "qbagging",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bagging:**\n",
    "\n",
    "The first Ensemble technique we deal with is called _Bagging_ (Bootstrap AGGregatING).\n",
    "Bagging consists of training a number of weak learners using randomly sampled instances from our data (with replacement). We have to start\n",
    "by choosing the number of estimators we want to use. Then for each estimator, we sample a random subset of the data to fit the estimator.\n",
    "\n",
    "To compute the prediction, we sum the prediction probabilities of the estimators and return the label that has the highest\n",
    "accumulated probabilities.\n",
    "\n",
    "- 4.4 [3 points] First, complete `sample_data` to return a random sample of size `sample_ratio* len(X_train)` of features and labels\n",
    "\n",
    "- 4.5 [4 points] Complete `fit` by instantiating `n_estimators` of our weak leaner, each trained on random sample of the data\n",
    "\n",
    "- 4.6 [4 points] Complete `predict` method to return the most likely label by combining different estimators predictions.\n",
    "Instead of the majority vote used in KNNClassifier, you should use `predict_proba` method of DecisionTreeClassifier.\n",
    "[See Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[   39, 38000,     0,     1],\n",
    "              [   48, 49000,     0,     0],\n",
    "              [   38, 41000,     0,     1],\n",
    "              [   38, 45000,     0,     0],])\n",
    "y = np.array([1,1,1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51ea23f426fd6ed1b0d6c9ccd3da038b",
     "grade": true,
     "grade_id": "abagging",
     "locked": false,
     "points": 11,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BaggingEnsemble(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Initialize BaggingEnsemble\n",
    "        :param n_estimators: number of estimators/weak learner to use\n",
    "        :param sample_ratio: ratio of the training data to sample\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.estimators = [] # List used in fit method to store the trained estimators\n",
    "\n",
    "    def sample_data(self, X_train, y_train):\n",
    "        X_sample, y_sample = None, None\n",
    "        #Workspace 4.4\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train),\n",
    "        #     sampling with replacement (iid)\n",
    "        #BEGIN \n",
    "        x_int = np.random.choice(X_train.shape[0], int(self.sample_ratio* len(X_train)))\n",
    "        X_sample = X_train[x_int, :]\n",
    "        y_sample = y_train[x_int]\n",
    "        #END\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the different estimators on sampled data using provided training samples\n",
    "        :param X_train: training samples, shape (num_samples, num_features)\n",
    "        :param y_train: training labels, shape (num_samples)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        np.random.seed(42) # Keep it to get consistent results across runs, you can change the seed value\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            #Workspace 4.5\n",
    "            #BEGIN \n",
    "            X_sample, y_sample =  self.sample_data(X_train, y_train)\n",
    "            ith_weak_clf = get_weak_learner().fit(X_sample, y_sample)\n",
    "            self.estimators.append(ith_weak_clf)\n",
    "            #END\n",
    "        return self\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of test samples\n",
    "        :param X_test: array of shape (num_points, num_features)\n",
    "        :return: 1-d array of shape (num_points)\n",
    "        \"\"\"\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #Workspace 4.6\n",
    "    #TODO: go through the trained estimators and accumulate their predicted_proba\n",
    "    #      to get the mostly likely label\n",
    "        #BEGIN \n",
    "        nth_accu = []\n",
    "        most_likely_proba = []\n",
    "        # loop through all observations\n",
    "        for nth_obs in range(X_test.shape[0]):\n",
    "            # loop through each estimator \n",
    "            for model in self.estimators:\n",
    "                # get predictied proba of each estimator on X_test\n",
    "                predicted_proba = model.predict_proba(X_test)\n",
    "                # append calculated predicted proba of each estimator on the nth observation \n",
    "                nth_accu.append(predicted_proba[nth_obs])\n",
    "            # go through each estimator for nth observation and accumulate their predicted proba\n",
    "            accumulated_nth_proba = np.einsum('ij->j', nth_accu)\n",
    "            # append to list that contains the accumulated proba of nth observations \n",
    "            most_likely_proba.append(accumulated_nth_proba)\n",
    "            # reset to obtain next nth predicted proba \n",
    "            nth_accu = []\n",
    "        # get the most likely lable for each observation\n",
    "        answer = np.argmax(most_likely_proba, axis=1) \n",
    "        #END\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "          Model  Precision(weighted)  Mean accuracy  Execution time\n",
      "3  RandomForest             0.700115       0.551020        2.066982\n",
      "1        Scikit             0.685739       0.680272        0.002921\n",
      "2       Bagging             0.659922       0.639456        0.126725\n",
      "0            WL             0.639790       0.625850        0.004021\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB8AAAF1CAYAAACgb0IzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2hUlEQVR4nO3df5weZX3v/9ebACqCUmG1CkQ4FrVpK6muoGgFWrUBpKmtHsFf1aoprdRjq1b0tFr121alx9YKmpMixZ9Qf6FRww+1KhZEE2z4DRojmhiPBFB+ScXA5/vHzMLNcm+yye7svXvv6/l43I975pprZj4z7F4ZPntd16SqkCRJkiRJ6spOgw5AkiRJkiQNN5MPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOmXyQdqKJG9Mcuqg45AkSZKmQ5IXJDlvhs/5W0mumclzavYx+aA5Kcm1Se5Isve48rVJKsn+29j/8CQbt3Weqvr7qnr5FMOVpKGT5KlJLkxyU5Ibk1yQ5ImDjkuSBq19Tr09ya09n5MHFMv+7bPxzmNlVfWRqnrmNJ/nBT3XenuSu3qvv6q+VlWPmc5zau4x+aC57HvAcWMrSX4DeMB0Hby3kZYk3SPJg4DPAe8BHgLsA7wF+Pk0nmPBdB1LkgbgmKravedzwqAD6lKb0Ni9qnYHjgQ29V7/oOPT7GDyQXPZh4AX96z/EfDBsZUk90vyj0l+kOTHSZYneUCSBwJnA4/oycg+IsnfJvlEkg8nuRl4SVv24Z5jjv2l76dJNiR5yQxdqyTNJo8GqKozqurOqrq9qs6rqksBkrwiyVVJbklyZZLHt+W/muQrbRt6RZLfGztgktOTvC/JqiS3AUe0bfMnk2xO8r0kr+qpf3CSNUlubtv4d83wPZCk7da2c5/oWX9Hki+l0ffZtafu0raX781JvptkSVt+bZKn99TrfX49v/3+afvM++QkL0nynz31D02yuu3JtjrJoT3bvpLkbW3vtluSnDe+5/Ekr/tevY7bmF+X5NIktyV5f5KHJTm7Pc8Xk/xST/0n9TyDX5Lk8O2NQYNn8kFz2UXAg9qH2QXA84AP92x/B80D8mLgV2j+MvemqrqN+2ZkN7X7LAU+AewJfKT3ZEkW0iQt3gOMtMdd28WFSdIs923gziQfSHLkuAfE5wJ/S5McfhDwe8ANSXYBPgucBzwU+HPgI0l6u+E+H/g7YA/gwrb+JTTt9+8Ar07yu23ddwPvrqoHAY8CPtbRtUrSdHoN8Lg2AfBbwMuAP6qqYoJnV2gSrjR/ZHsdzXPq04BrJ3G+p7Xfe7bPvF/v3ZjkIcDngX8B9gLeBXw+yV491Z4PvJSm7d4VeO12XfHE/hB4Bs01H0PznP1GYG+a/099VRvjPm2M/x9Nb7vXAp9MMjJNcWiGmHzQXDfW++EZwNXAD9vyAK8A/qKqbqyqW4C/B47dxvG+XlWfrqq7qur2cdteAHyx/UvfL6rqhqpaO21XIklzRFXdDDwVKOBfgc1JViZ5GPBy4J1Vtboa66rq+8CTgN2Bt1fVHVX1HzRDN47rOfRnquqCqroL+A1gpKre2tZf355rrB3/BfArSfauqlur6qKZuHZJmqRPt3+lH/u8AqCqfga8kOZ/8j8M/HlVbUyyrWfXlwGnVdUX2ufUH1bV1dMQ59HAd6rqQ1W1parOoHmmPqanzr9V1bfbZ+OP0SRHpsN7qurHVfVD4GvAN6rqv6rq58BZwG+29V4IrKqqVe21fwFYAxw1TXFohjimXXPdh2i6kx1Az5ALmp4JuwEXN2050CQktjWGeMNWtu0HfHfHwpSk4VJVVwEvAUjyWJqH6H9m4rbyEcCGNrEw5vs0f9kb09sGP5JmeNxPe8oW0DygQvMg/lbg6iTfA95SVZ/bwcuRpOn2+1X1xX4bquqbSdbT9CQY67W1rWfX/YBVHcT5CJq2uNf4tvn/9Sz/jCaRPB1+3LN8e5/1sfM8Enhukt6EyC7Al6cpDs0Qkw+a06rq++1D51E0D6JjrqdptH6tzabeZ9eJDrmV020ADt6hQCVpiFXV1UlOB/6Epq18VJ9qm4D9kuzUk4BYSDOE4+5D9SxvAL5XVQdOcM7vAMcl2Qn4A+ATSfZqh9ZJ0qyV5JXA/Wjaxb8C/oFtP7tO1LYC3EaTuBjzyz3LW3u2pY3hkePKFgLnbGO/mbQB+FBVvWLQgWhqHHahYfAy4LfHPXDeRdM995+SPBSa8WI9Y4V/DOyV5MHbcZ6PAE9P8j+T7JxkrySLpyF+SZpTkjw2yWuS7Nuu70czfOIi4FTgtUme0E6g9itJHgl8g+YB+a+S7NJOFnYMcOYEp/kmcHOS16eZLHhBkl9P+zrPJC9MMtImMn7a7nNnN1csSdMjyaNp5i54IfAimjZxcduWbe3Z9f3AS5P8TpKd2m2PbbetBY5t29ZR4Dk9p9xM81z8PyYIaRXw6CTPb59vnwcsohkWN1t8GDgmye+2/xbcv53Act9BB6btY/JBc15Vfbeq1vTZ9HpgHXBRmrdXfBF4TLvP1cAZwPp2HN4jJnGeH9D0sHgNcCNNQ3/QtFyEJM0ttwCHAN9I82aKi4DLgddU1cdpJo38aFvv08BDquoOmsknj6T5C997gRdPNGa5qu6kSU4spnm18vU0iY2xpPES4Iokt9JMPnlsVf33tF+pJO2Yz+aet6rdmuSsNK9x/zDwjqq6pO3B9UbgQ0nux9afXb9JM+njPwE3AV/lnh4Lf0PTK+InNK89/uhYEO0cE38HXNA+8z6pN8iqugF4Fs3z7Q00PTGeVVXXT/8t2TFVtYFmUvg30iRTNtBMvOn/y84xaSZWlSRJkiRJ6obZIkmSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ3aedABbK+999679t9//0GHIUn3cfHFF19fVSODjmMm2BZLmo1shyVp8CZqi+dc8mH//fdnzZo1gw5Dku4jyfcHHcNMsS2WNBvZDkvS4E3UFjvsQpIkSZIkdarT5EOSJUmuSbIuyYl9tr8uydr2c3mSO5M8pMuYJEmSJEnSzOos+ZBkAXAKcCSwCDguyaLeOlV1UlUtrqrFwBuAr1bVjV3FJEmSJEmSZl6XPR8OBtZV1fqqugM4E1i6lfrHAWd0GI8kSZIkSRqALpMP+wAbetY3tmX3kWQ3YAnwyQm2L0uyJsmazZs3T3ugkiRJkiSpO10mH9KnrCaoewxwwURDLqpqRVWNVtXoyMi8eHuSJEmSJElDo8vkw0Zgv571fYFNE9Q9FodcSJIkSZI0lLpMPqwGDkxyQJJdaRIMK8dXSvJg4DDgMx3GIkmSJEmSBmTnrg5cVVuSnACcCywATquqK5Ic325f3lZ9NnBeVd3WVSySJEmSJGlwOks+AFTVKmDVuLLl49ZPB07vMg5JkiRJkjQ4XQ67kCQNWJIlSa5Jsi7JiX22vy7J2vZzeZI7kzxkELFKkiRpeJl8kKQhlWQBcApwJLAIOC7Jot46VXVSVS2uqsXAG4CvTvTmIUmSJGlHmXyQpOF1MLCuqtZX1R3AmcDSrdQ/Dt88JEmSpA50OueDZr/9T/z8oEPo3LVvP3rQIUiDsg+woWd9I3BIv4pJdgOWACdMdLAky4BlAAsXLpy+KCVtN//9lqTBG/a2eLrbYXs+SNLwSp+ymqDuMcAFWxtyUVUrqmq0qkZHRkamJUBJkiTNDyYfJGl4bQT261nfF9g0Qd1jcciFJEmSOmLyQZKG12rgwCQHJNmVJsGwcnylJA8GDgM+M8PxSZIkaZ5wzgdJGlJVtSXJCcC5wALgtKq6Isnx7fblbdVnA+dV1W0DClWSJElDzuSDJA2xqloFrBpXtnzc+unA6TMXlSRJkuYbh11IkiRJkqRO2fNBkqQB81Vd0tyXZD/gg8AvA3cBK6rq3ePqBHg3cBTwM+AlVfWtdtuSdtsC4NSqevsMhi9JnbPngyRJkjR1W4DXVNWvAk8CXplk0bg6RwIHtp9lwPsAkiwATmm3LwKO67OvJM1pJh8kSZKkKaqqH431YqiqW4CrgH3GVVsKfLAaFwF7Jnk4cDCwrqrWV9UdwJltXUkaGiYfJEmSpGmUZH/gN4FvjNu0D7ChZ31jWzZR+fjjLkuyJsmazZs3T2vMktQ1kw+SJEnSNEmyO/BJ4NVVdfP4zX12qa2U37ugakVVjVbV6MjIyNSDlaQZ5ISTkiRJ0jRIsgtN4uEjVfWpPlU2Avv1rO8LbAJ2naBckoaGPR8kSZKkKWrfZPF+4KqqetcE1VYCL07jScBNVfUjYDVwYJIDkuwKHNvWlaShYc8HSZIkaeqeArwIuCzJ2rbsjcBCgKpaDqyiec3mOppXbb603bYlyQnAuTSv2jytqq6Y0eglqWMmHyRJkqQpqqr/pP/cDb11CnjlBNtW0SQnJGkoOexCkiRJkiR1yuSDJEmSJEnqlMMuJEnSrLT/iZ8fdAidu/btRw86BEmSZoTJB2kCPvRKkiRJ0vRw2IUkSZIkSeqUyQdJkiRJktQpkw+SJEmSJKlTJh8kSZIkSVKnTD5IkiRJkqROzZu3XQz7mwt8a4EkSZIkabay54MkSZIkSerUvOn5IEkarGHvgQb2QpMkSZqIPR8kSZIkSVKnTD5IkiRJkqROmXyQJEmSJEmdMvkgSZIkSZI6ZfJBkiRJkiR1qtPkQ5IlSa5Jsi7JiRPUOTzJ2iRXJPlql/FIkiRJkqSZ19mrNpMsAE4BngFsBFYnWVlVV/bU2RN4L7Ckqn6Q5KFdxSNJkiRJkgajy54PBwPrqmp9Vd0BnAksHVfn+cCnquoHAFV1XYfxSJIkSZKkAegy+bAPsKFnfWNb1uvRwC8l+UqSi5O8uMN4JEmSJEnSAHQ27AJIn7Lqc/4nAL8DPAD4epKLqurb9zpQsgxYBrBw4cIOQpUkSZIkSV3psufDRmC/nvV9gU196pxTVbdV1fXA+cBB4w9UVSuqarSqRkdGRjoLWJIkSZIkTb8ukw+rgQOTHJBkV+BYYOW4Op8BfivJzkl2Aw4BruowJkmSJEmSNMM6G3ZRVVuSnACcCywATquqK5Ic325fXlVXJTkHuBS4Czi1qi7vKiZJkiRJkjTzupzzgapaBawaV7Z83PpJwEldxiFJ81WSJcC7aZLAp1bV2/vUORz4Z2AX4PqqOmwGQ5QkSdI80GnyQZI0OEkWAKcAz6CZY2d1kpVVdWVPnT2B9wJLquoHSR46kGAlSZI01Lqc80GSNFgHA+uqan1V3QGcCSwdV+f5wKeq6gcAVXXdDMcoSZKkecDkgyQNr32ADT3rG9uyXo8GfinJV5JcnOTFMxadJA2RJKcluS5J3/nLkrwuydr2c3mSO5M8pN12bZLL2m1rZjZySZoZJh8kaXilT1mNW98ZeAJwNPC7wN8keXTfgyXLkqxJsmbz5s3TG6kkzX2nA0sm2lhVJ1XV4qpaDLwB+GpV3dhT5Yh2+2i3YUrSYJh8kKThtRHYr2d9X2BTnzrnVNVtVXU9cD5wUL+DVdWKqhqtqtGRkZFOApakuaqqzgdu3GbFxnHAGR2GI0mzjskHSRpeq4EDkxyQZFfgWGDluDqfAX4ryc5JdgMOAa6a4Tglad5o29olwCd7igs4rx3+tmwwkUlSt3zbhSQNqarakuQE4FyaV22eVlVXJDm+3b68qq5Kcg5wKXAXzes4+45XliRNi2OAC8YNuXhKVW1q3zj0hSRXtz0p7qVNTCwDWLhw4cxEK0nTxOSDJA2xqloFrBpXtnzc+knASTMZlyTNY8cybshFVW1qv69LchbN24ruk3yoqhXACoDR0dHxc/hI0qzmsAtJkiRpBiR5MHAYzZC3sbIHJtljbBl4JmAPNElDx54PkiRJ0hQlOQM4HNg7yUbgzcAucK8eZ88Gzquq23p2fRhwVhJons0/WlXnzFTckjRTTD5IkiRJU1RVx02izuk0r+TsLVvPBG8ZkqRh4rALSZIkSZLUKZMPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOuWrNiVtt/1P/PygQ+jUtW8/etAhSJIkSUPFng+SJEmSJKlTJh8kSZIkSVKnTD5IkiRJkqROmXyQJEmSJEmdMvkgSZIkSZI6ZfJBkiRJkiR1yuSDJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOmXyQZIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE51mnxIsiTJNUnWJTmxz/bDk9yUZG37eVOX8UiSJEmSpJm3c1cHTrIAOAV4BrARWJ1kZVVdOa7q16rqWV3FIUmSJEmSBqvLng8HA+uqan1V3QGcCSzt8HySJEmSJGkW6jL5sA+woWd9Y1s23pOTXJLk7CS/1u9ASZYlWZNkzebNm7uIVZIkSdphSU5Lcl2SyyfYPuFw420NVZakYdBl8iF9ymrc+reAR1bVQcB7gE/3O1BVraiq0aoaHRkZmd4oJUmSpKk7HViyjTpfq6rF7eetcK+hykcCi4DjkizqNFJJGoAukw8bgf161vcFNvVWqKqbq+rWdnkVsEuSvTuMSZIkSZp2VXU+cOMO7OpQZUnzQpfJh9XAgUkOSLIrcCywsrdCkl9Oknb54DaeGzqMSZIkSRqUfsONJztU2aHIkua0zpIPVbUFOAE4F7gK+FhVXZHk+CTHt9WeA1ye5BLgX4Bjq2r80AxJ0g7ylceSNGtMNNx4MkOVm0KHIkuawzp71SbcPZRi1biy5T3LJwMndxmDJM1XvvJYkmaPqrq5Z3lVkve2w423OVRZkoZBl8MuJEmD5ThiSZoltjLceJtDlSVpGHTa80GSNFD9xhEf0qfek9vhb5uA11bVFf0OlmQZsAxg4cKF0xyqJM1tSc4ADgf2TrIReDOwC9zd8/c5wJ8m2QLczj3DjbckGRuqvAA4baJ2WJLmMpMPkjS8tueVx7cmOYpmDPKB/Q5WVSuAFQCjo6POzyNJParquG1sn3C4cb+hypI0bBx2IUnDy1ceS5IkaVYw+SBJw8tXHkuSJGlWcNiFJA2pquo7jnjsdcfbGIMsSZIkTRuTD5I0xHzlsSRJkmYDh11IkiRJkqROmXyQJEmSJEmdMvkgSZIkSZI6ZfJBkiRJkiR1yuSDJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOmXyQZIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ0y+SBJkiRJkjpl8kGSJEmSJHXK5IMkSZIkSeqUyQdJkiRJktQpkw+SJEmSJKlTJh8kSZIkSVKnTD5IkiRJkqROmXyQJEmSpijJaUmuS3L5BNtfkOTS9nNhkoN6tl2b5LIka5OsmbmoJWnmmHyQJEmSpu50YMlWtn8POKyqHge8DVgxbvsRVbW4qkY7ik+SBmrnQQcgSZIkzXVVdX6S/bey/cKe1YuAfTsPSpJmEXs+SJIkSTPrZcDZPesFnJfk4iTLJtopybIka5Ks2bx5c+dBStJ0sueDJEmSNEOSHEGTfHhqT/FTqmpTkocCX0hydVWdP37fqlpBO1xjdHS0ZiRgSZomnfZ8SLIkyTVJ1iU5cSv1npjkziTP6TIeSZIkaVCSPA44FVhaVTeMlVfVpvb7OuAs4ODBRChJ3eks+ZBkAXAKcCSwCDguyaIJ6r0DOLerWCRJkqRBSrIQ+BTwoqr6dk/5A5PsMbYMPBPo+8YMSZrLtjnsIsmzgFVVddd2HvtgYF1VrW+PcyawFLhyXL0/Bz4JPHE7jy9JkiTNCknOAA4H9k6yEXgzsAtAVS0H3gTsBbw3CcCW9s0WDwPOast2Bj5aVefM+AVIUscmM+fDscC7k3wS+LequmqSx94H2NCzvhE4pLdCkn2AZwO/zVaSD+3EO8sAFi5cOMnTS5IkSTOjqo7bxvaXAy/vU74eOKiruCRpttjmsIuqeiHwm8B3gX9L8vV2pt09trFr+h1u3Po/A6+vqju3EcOKqhqtqtGRkZFthSxJQynJU5J8Icm3k6xP8r0k6wcdlyRJkrQtk5rzoapuphkacSbwcJreCt9K8udb2W0jsF/P+r7ApnF1RoEzk1wLPIemG9rvTypySZp/3g+8i2aG9CfStKFbHbLmxL+SJEmaDSYz58MxwB8DjwI+BBxcVdcl2Q24CnjPBLuuBg5McgDwQ5rhG8/vrVBVB/Sc53Tgc1X16e2/DEmaF26qqrO3Xa3RM/HvM2gSwquTrKyqK/vUc+JfSZIkdWYycz48F/in8e8arqqfJfnjiXaqqi1JTqB5mF0AnFZVVyQ5vt2+fApxS9J89OUkJ9HMlv7zscKq+tYE9Z34V5IkSbPCZJIPbwZ+NLaS5AHAw6rq2qr60tZ2rKpVwKpxZX2TDlX1kknEIknz2dikvaM9ZUUzaW8/0zbxb1vXyX8lSZK0QyaTfPg4cGjP+p1tmX8hk6QZVFVHbOcu2zXxb/uat62dfwWwAmB0dHT8cSRJkqQJTSb5sHNV3TG2UlV3JNm1w5gkSX0keTBNb7SntUVfBd5aVTdNsMv2TPwLsDdwVJItzr8jSZKk6TSZt11sTvJ7YytJlgLXdxeSJGkCpwG3AP+z/dwM/NtW6t898W+bND4WWNlboaoOqKr9q2p/4BPAn5l4kCRJ0nSbTM+H44GPJDmZpgvvBuDFnUYlSernUVX1hz3rb0mydqLKTvwrSdsvycOAvwceUVVHJlkEPLmq3j/g0CRpTttm8qGqvgs8KcnuQKrqlu7DkiT1cXuSp1bVfwIkeQpw+9Z2cOJfSdpup9P0Kvvf7fq3gX8HTD5I0hRMpucDSY4Gfg24/9iEZFX11g7jkiTd158CH2jnfghwI/CSgUYkScNn76r6WJI3wN29yO4cdFCSNNdtM/mQZDmwG3AEcCrwHOCbHcclSRqnqtYCByV5ULt+82AjkqShdFuSvWjfDpTkScBEE/tKkiZpMj0fDq2qxyW5tKrekuT/AJ/qOjBJUiPJC6vqw0n+clw5AFX1roEEJknD6S9pJud9VJILgBGaP75JkqZgMsmH/26/f5bkEcANwAHdhSRJGueB7fceA41CkuaBqvpWksOAx9AMcbumqn4x4LAkac6bTPLhs0n2BE4CvkXTBe1fuwxKknSPqvq/7fdbBh2LJA27JAuAo4D9aZ6Vn5nEXmaSNEU7bW1jkp2AL1XVT6vqk8AjgcdW1ZtmJDpJ0t2SvDPJg5LskuRLSa5P8sJBxyVJQ+azNJP57kXT42zsI0magq32fKiqu9o5Hp7crv8c+PlMBCZJuo9nVtVfJXk2sBF4LvBl4MODDUuShsq+VfW4QQchScNmqz0fWucl+cOMzWwmSRqUXdrvo4AzqurGQQYjSUPq7CTPHHQQkjRsJjPnw1/STHa2Jcl/00y8U1X1oE4jkySN99kkVwO3A3+WZIR7JgWWJE2Pi4Cz2uHHv8BnX0maFttMPlSVY9wkaRaoqhOTvAO4uaruTHIbsHTQcUnSkBkbcnxZVdWgg5GkYbHN5EOSp/Urr6rzpz8cSdJ4SX67qv4jyR/0lPVW+dTMRyVJQ+s7wOUmHiRpek1m2MXrepbvDxwMXAz8dicRSZLGOwz4D+CYPtsKkw+SNJ1+BHwlydn0TLTuqzYlaWomM+ziXg+7SfYD3tlZRJKke6mqN7ffLx10LJI0D3yv/ezafiRJ02AyPR/G2wj8+nQHIknauiR/D7yzqn7arv8S8Jqq+uuBBiZJQ6Sq3jLoGCRpGE1mzof30HTrhebVnIuBSzqMSZLU35FV9caxlar6SZKjAJMPkjRFSU6uqhOSfJZ7nn3vVlW/N4CwJGloTKbnw5qe5S0075a/oKN4JEkTW5DkflX1c4AkDwDuN+CYJGlYvBg4AfjHQQciScNoMsmHTwD/XVV3AiRZkGS3qvpZt6FJksb5MPClJP9G81e5PwY+MNiQJGlofBegqr466EAkaRhNJvnwJeDpwK3t+gOA84BDuwpKknRfVfXOJJfStMkB3lZV5w44LEkaFiNJ/nKijb7tQpKmZqdJ1Ll/VY0lHmiXd+suJEnSVlwFnFNVrwG+lmSPQQckSUNiAbA7sMcEn61KclqS65JcPsH2JPmXJOuSXJrk8T3bliS5pt124rRcjSTNMpPp+XBbksdX1bcAkjwBuL3bsCRJ4yV5BbAMeAjwKGAfYDnwO4OMS5KGxI+q6q1T2P904GTggxNsPxI4sP0cArwPOCTJAuAU4Bk0b5VbnWRlVV05hVgkadaZTPLh1cDHk2xq1x8OPK+ziCRJE3klcDDwDYCq+k6Shw42JEkaGpnKzlV1fpL9t1JlKfDBqirgoiR7Jnk4sD+wrqrWAyQ5s61r8kHSUNlm8qGqVid5LPAYmkb56qr6ReeRSZLG+3lV3ZE0z8dJdqbP6+AkSTuk615k+wAbetY3tmX9yg/pOBZJmnHbnPMhySuBB1bV5VV1GbB7kj/rPjRJ0jhfTfJG4AFJngF8HPjsgGOSpKFQVTd2fIp+PStqK+X3PUCyLMmaJGs2b948rcFJUtcmM+HkK6rqp2MrVfUT4BWdRSRJmsjrgc3AZcCfAKuAvx5oRJKkydoI7Nezvi+waSvl91FVK6pqtKpGR0ZGOgtUkrowmTkfdkqSdnwa7aQ4u3YbliSpV5KdgEur6teBfx10PJKk7bYSOKGd0+EQ4Kaq+lGSzcCBSQ4AfggcCzx/gHFKUicmk3w4F/hYkuU0XcCOB87uNCpJ0r1U1V1JLkmysKp+MOh4JEn3luQM4HBg7yQbgTcDuwBU1XKa3mpHAeuAnwEvbbdtSXICzTP3AuC0qrpixi9Akjo2meTD62le7fanNGPS/ovmjReSpJn1cOCKJN8EbhsrrKrfG1xIkiSAqjpuG9uL5q1F/batoklOSNLQmszbLu5KchHwP2hesfkQ4JNdByZJuo+3DDoASZIkaUdMmHxI8miaMWfHATcA/w5QVUfMTGiSJIAk96cZ8vYrNJNNvr+qtgw2KkmSJGnyttbz4Wrga8AxVbUOIMlfzEhUkqReHwB+QdMmHwksAv7XQCOSJEmStsPWkg9/SNPz4ctJzgHOpP97iCVJ3VpUVb8BkOT9wDcHHI8kSZK0XXaaaENVnVVVzwMeC3wF+AvgYUnel+SZkzl4kiVJrkmyLsmJfbYvTXJpkrVJ1iR56g5ehyQNs1+MLTjcQpIkSXPRhMmHMVV1W1V9pKqeBewLrAXuk0gYL8kC4BTu6SJ8XJJF46p9CTioqhYDfwycul3RS9L8cFCSm9vPLcDjxpaT3Dzo4CRJkqRtmcyrNu9WVTcC/7f9bMvBwLqqWg+Q5ExgKXBlz/Fu7an/QKC2Jx5Jmg+qasGgY5AkSZKmYps9H6ZgH2BDz/rGtuxekjw7ydXA52l6P0iSJEmSpCHSZfKh3+SU9+nZ0M4t8Vjg94G39T1QsqydE2LN5s2bpzdKSRpizr0jSZKk2aDL5MNGYL+e9X2BTRNVrqrzgUcl2bvPthVVNVpVoyMjI9MfqSQNIefekSRJ0mzRZfJhNXBgkgOS7Erz2s6VvRWS/EqStMuPB3YFbugwJkmaT+6ee6eq7qB5ZfLS3gpVdWtVjfVKc+4dSZIkdWK7JpzcHlW1JckJwLnAAuC0qroiyfHt9uXAHwIvTvIL4HbgeT0PwZKkqek3984h4ysleTbwD8BDgaNnJjRJkiTNJ50lHwCqahWwalzZ8p7ldwDv6DIGSZrHJj33DnBWkqfRzL3z9L4HS5YBywAWLlw4jWFKkiRp2HU57EKSNFjTNvdOu935dyRJkrRDTD5I0vBy7h1JkiTNCp0Ou5AkDY5z70iSJGm2MPkgSUPMuXckSZI0GzjsQpIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ0y+SBJkiRJkjpl8kGSJEmSJHXK5IMkSZIkSeqUyQdJkiRJktQpkw+SJEmSJKlTJh8kSZKkKUqyJMk1SdYlObHP9tclWdt+Lk9yZ5KHtNuuTXJZu23NzEcvSd3bedABSJIkSXNZkgXAKcAzgI3A6iQrq+rKsTpVdRJwUlv/GOAvqurGnsMcUVXXz2DYkjSj7PkgSZIkTc3BwLqqWl9VdwBnAku3Uv844IwZiUySZgmTD5IkSdLU7ANs6Fnf2JbdR5LdgCXAJ3uKCzgvycVJlk10kiTLkqxJsmbz5s3TELYkzRyTD5IkSdLUpE9ZTVD3GOCCcUMunlJVjweOBF6Z5Gn9dqyqFVU1WlWjIyMjU4tYkmaYyQdJkiRpajYC+/Ws7wtsmqDusYwbclFVm9rv64CzaIZxSNJQMfkgSZIkTc1q4MAkByTZlSbBsHJ8pSQPBg4DPtNT9sAke4wtA88ELp+RqCVpBvm2C0mSJGkKqmpLkhOAc4EFwGlVdUWS49vty9uqzwbOq6rbenZ/GHBWEmiezT9aVefMXPSSNDNMPkiSJElTVFWrgFXjypaPWz8dOH1c2XrgoI7Dk6SBc9iFJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOmXyQZIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqc6TT4kWZLkmiTrkpzYZ/sLklzafi5MclCX8UiSJEmSpJnXWfIhyQLgFOBIYBFwXJJF46p9Dzisqh4HvA1Y0VU8kiRJkiRpMLrs+XAwsK6q1lfVHcCZwNLeClV1YVX9pF29CNi3w3gkSZIkSdIAdJl82AfY0LO+sS2byMuAs/ttSLIsyZokazZv3jyNIUrScHP4myRJkmaDLpMP6VNWfSsmR9AkH17fb3tVraiq0aoaHRkZmcYQJWl4OfxNkiRJs0WXyYeNwH496/sCm8ZXSvI44FRgaVXd0GE8kjTfOPxNkiRJs0KXyYfVwIFJDkiyK3AssLK3QpKFwKeAF1XVtzuMRZLmo2kb/gYOgZMkSdKO27mrA1fVliQnAOcCC4DTquqKJMe325cDbwL2At6bBGBLVY12FZMkzTM7MvztqRMdrKpW0A7LGB0d7XscSZIkqZ/Okg8AVbUKWDWubHnP8suBl3cZgyTNY9s7/O1Ih79JkiSpC10Ou5AkDZbD3yRJkjQrdNrzQZI0OA5/kyRJ0mxh8kGShpjD3yRJkjQbOOxCkiRJkiR1yuSDJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpUyYfJEmSpClKsiTJNUnWJTmxz/bDk9yUZG37edNk95WkYeDbLiRJkqQpSLIAOAV4BrARWJ1kZVVdOa7q16rqWTu4ryTNafZ8kCRJkqbmYGBdVa2vqjuAM4GlM7CvJM0ZJh8kSZKkqdkH2NCzvrEtG+/JSS5JcnaSX9vOfSVpTnPYhSRJkjQ16VNW49a/BTyyqm5NchTwaeDASe7bnCRZBiwDWLhw4Q4HK0mDYM8HSZIkaWo2Avv1rO8LbOqtUFU3V9Wt7fIqYJcke09m355jrKiq0aoaHRkZmc74JalzJh8kSZKkqVkNHJjkgCS7AscCK3srJPnlJGmXD6Z5Dr9hMvtK0jBw2IUkSZI0BVW1JckJwLnAAuC0qroiyfHt9uXAc4A/TbIFuB04tqoK6LvvQC5Ekjpk8kGSJEmaonYoxapxZct7lk8GTp7svpI0bBx2IUmSJEmSOmXyQZIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ0y+SBJkiRJkjpl8kGSJEmSJHXK5IMkSZIkSeqUyQdJkiRJktQpkw+SJEmSJKlTJh8kSZIkSVKnTD5IkiRJkqROmXyQJEmSJEmdMvkgSZIkSZI6ZfJBkiRJkiR1yuSDJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpU50mH5IsSXJNknVJTuyz/bFJvp7k50le22UskiRJkiRpMDpLPiRZAJwCHAksAo5LsmhctRuBVwH/2FUckjSfmQSWJEnSbNBlz4eDgXVVtb6q7gDOBJb2Vqiq66pqNfCLDuOQpHnJJLAkSZJmiy6TD/sAG3rWN7Zl2y3JsiRrkqzZvHnztAQnSfOASWBJkiTNCl0mH9KnrHbkQFW1oqpGq2p0ZGRkimFJ0rwxbUlgMBEsSZKkHddl8mEjsF/P+r7Apg7PJ0m6t2lLAoOJYEmSJO24LpMPq4EDkxyQZFfgWGBlh+eTJN2bSWBJkiTNCjt3deCq2pLkBOBcYAFwWlVdkeT4dvvyJL8MrAEeBNyV5NXAoqq6uau4JGkeuTsJDPyQJgn8/MGGJEmSpPmos+QDQFWtAlaNK1ves/z/aP4SJ0maZiaBJWnmJFkCvJumvT21qt4+bvsLgNe3q7cCf1pVl7TbrgVuAe4EtlTV6EzFLUkzpdPkgyRpsEwCS1L3el5t/AyaIW+rk6ysqit7qn0POKyqfpLkSGAFcEjP9iOq6voZC1qSZliXcz5IkiRJ88FkXm18YVX9pF29CBO/kuYZkw+SJEnS1Gzvq41fBpzds17AeUkuTrJsop185bGkucxhF5IkSdLUTPrVxkmOoEk+PLWn+ClVtSnJQ4EvJLm6qs6/zwGrVtAM12B0dHSHX50sSYNgzwdJkiRpaib1auMkjwNOBZZW1Q1j5VW1qf2+DjiLZhiHJA0Vkw+SJEnS1Nz9auMku9K82nhlb4UkC4FPAS+qqm/3lD8wyR5jy8AzgctnLHJJmiEOu5AkSZKmYDKvNgbeBOwFvDcJ3PNKzYcBZ7VlOwMfrapzBnAZktQpkw+SJEnSFE3i1cYvB17eZ7/1wEGdByhJA+awC0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ0y+SBJkiRJkjpl8kGSJEmSJHXK5IMkSZIkSeqUyQdJkiRJktQpkw+SJEmSJKlTJh8kSZIkSVKnTD5IkiRJkqROmXyQJEmSJEmd2nnQAUiSJEmSZqf9T/z8oEPo1LVvP3rQIcwb9nyQJEmSJEmdMvkgSZIkSZI6ZfJBkiRJkiR1yuSDJEmSJEnqlMkHSZIkSZLUKZMPkiRJkiSpUyYfJEmSJElSp0w+SJIkSZKkTpl8kCRJkiRJnTL5IEmSJEmSOmXyQZIkSZIkdcrkgyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVOdJh+SLElyTZJ1SU7ssz1J/qXdfmmSx3cZjyTNN7bDkjQzptLebmtfSRoGnSUfkiwATgGOBBYBxyVZNK7akcCB7WcZ8L6u4pGk+cZ2WJJmxlTa20nuK0lzXpc9Hw4G1lXV+qq6AzgTWDquzlLgg9W4CNgzycM7jEmS5hPbYUmaGVNpbyezryTNeTt3eOx9gA096xuBQyZRZx/gRx3GJUnzhe2wJM2MqbS3k9l32ux/4ue7OvSscO3bj96h/Yb9vsCO3xtpunSZfEifstqBOiRZRtM9DeDWJNdMMbaZsDdw/UydLO+YqTNNC+9NfzN6X8B7M5Ep3JdHTmMY02Ha2mGYk22xv1MTmyu/U4Pgvelvrvw+Daodnkp7O8ztMPg7tTXem/68LxObK/emb1vcZfJhI7Bfz/q+wKYdqENVrQBWTHeAXUqypqpGBx3HbOS96c/7MjHvzQ6btnYY5l5b7M/NxLw3E/Pe9Od92aaptLe7TmJfYO61w+DPztZ4b/rzvkxsrt+bLud8WA0cmOSAJLsCxwIrx9VZCby4nf33ScBNVWVXX0maHrbDkjQzptLeTmZfSZrzOuv5UFVbkpwAnAssAE6rqiuSHN9uXw6sAo4C1gE/A17aVTySNN/YDkvSzJhKezvRvgO4DEnqVJfDLqiqVTQNbW/Z8p7lAl7ZZQwDNKe6xM0w701/3peJeW92kO2wJuC9mZj3pj/vyzZMpb3tt+8Q8WdnYt6b/rwvE5vT9yZNOyhJkiRJktSNLud8kCRJkiRJMvkwFUn+Kcmre9bPTXJqz/r/SfKXSS4fSIAzJMn/TnJFkkuTrE3S993USUaT/Eu7/LdJXtunzluTPL1dfnWS3bqNfmqS3Nle8yVJvpXk0A7Ocfd9m0k913Z5ks8m2XOajvuSJCdP07GuTXJZG+faLu5/e57FSY7q4tiaGtvhhu3wcLbD7blti+85j23xLGVb3LAtHs622Hb4XueZUjts8mFqLgQOBUiyE817V3+tZ/uhwAUDiGvGJHky8Czg8VX1OODpwIZ+datqTVW9amvHq6o3VdUX29VXA7O6oQVur6rFVXUQ8AbgH6b7BJO5bx0Zu7ZfB25k9s4LcEQb5+KqunAyOyTZ3vluFtNMEqbZx3bYdniY22GwLe61GNvi2cq22LZ4mNti2+F7LGYK7bDJh6m5gLahpWlgLwduSfJLSe4H/Crwk0EFN0MeDlxfVT8HqKrrq2pTkicmubDNfn4zyR5JDk/yufEHSPKKJGcneUCS05M8J8mrgEcAX07y5Rm+ph31INr/3kl2T/KlNvN7WZKlY5WS/E2Sq5N8IckZY9nu9p5dmuTrSU4a++tA731rs+OnJflKkvXtfdrqcafJ14F92vMc3P63/a/2+zFt+UuSfCrJOUm+k+SdPbG9NMm3k3wVeEpP+SPb+3Rp+72wLT89yfuSfLm9zsPa674qyelbC3Qbx3xX+/P0jiSPamO9OMnXkjy2rffcNrN9SZLz07z27K3A89pM8vOm8b5q6myHbYd7DXM7DLbFtsWzl22xbXGvYW6LbYen0g5XlZ8pfIBrgYXAnwDHA2+jyQY9BTgf2B+4fNBxdnj9uwNrgW8D7wUOA3YF1gNPbOs8iObNKocDn2vL/hZ4LXACzbus79eWnw48p+fe7j3oa9zG9d/ZXv/VwE3AE9rynYEHtct707xWK8BoW/8BwB7Ad4DXtvUuBw5tl98+9nPT575dCNyvPe4NwC5bO+4Uru3W9nsB8HFgSe9/z3b56cAn2+WXtP/dHwzcH/g+sB/NP8Y/AEban40LgJPbfT4L/FG7/MfAp3t+Ds5s79lS4GbgN2gSphcDi3t+Ri5rr/0bkzjm54AF7fqXgAPb5UOA/2iXLwP2aZf37Lm2kwf98+Znwp/Va7EdXovt8NC1w+35bItti+fEB9ti2+IhbYuxHYZpaoc7fdXmPDGW6T0UeBdNJuxQml+6SXV3mcuq6tYkTwB+CzgC+Hfg74AfVdXqts7NAEnG7/4iYCPw+1X1ixkLenrdXlWL4e7udh9M8us0DcTfJ3kacBfNz8XDgKcCn6mq29t9Ptt+7wnsUfd0kfooTde9fj5fTVb950mu29pxp+gBSdbSPCxcDHyhLX8w8IEkBwJF09CP+VJV3dTGcCXwSJp/EL5SVZvb8n8HHt3WfzLwB+3yh4B39hzrs1VVSS4DflxVl7X7X9HGtLatd0RVXd+z39aO+fGqujPJ7jS/px/v+bm8X/t9AXB6ko8Bn9rK/dHsYTtsO7wYhrIdBtti2+K5w7bYtngxDGVbbDs8Te2wwy6mbmyM22/QZOkuovkPPfRj28ZU1Z1V9ZWqejNN1vYPaH4Bt+Vyml+YfTsMb8ZU1ddpGpUR4AXt9xPahvjHNJnP+/xr05qovJ+f9yzfSZNR3p79J2vsH5FH0mRnx8a3vQ34cjXj3o6hua6txQaT+3kYX2/sWHeNO+5dPcfd3mPe1n7vBPy07hkXt7iqfhWgqo4H/pomQ702yV7bcS4Nhu2w7TAwlO0w2BbbFs8dtsW2xcBQtsW2w9PUDpt8mLoLaLJxN7YNzo3AnjSN7dcHGdhMSPKYNts3ZjFwFfCIJE9s6+yR/pOZ/BdN17yVSR7RZ/stNN2l5oR2fNQCmm5fDwauq6pfJDmCprEC+E/gmCT3bzONRwNU1U9oxkY+qa137Haevu9xp0ObtX0V8Noku9Bc2w/bzS+ZxCG+ARyeZK92/+f2bLuQe671BTTXMVXbPGb7l4fvJXkuQBoHtcuPqqpvVNWbgOtpGtw59bM4D9kO2w4Dw9sOt/HZFs+xn8d5yLbYthgY3rbYdnjq7bDJh6m7jCazd9G4spt6ur08JsnGns9z73OUuWt3mu5GVya5FFgEvAl4HvCeJJfQdE26f7+dq+o/aca5fT7J3uM2rwDOzuyeXOcBaV9pQ9O97o+q6k7gI8BokjU0v+xXA7Td7lYCl9B0X1pD0x0R4GXAiiRfp8na3sQkbeO4U1ZV/9Ue+1iaLlv/kOQCmn9YtrXvj2jG5X0d+CLwrZ7NrwJe2v7svAj4X9MQ7mSP+QLgZe3P6BU04+gATkozIdLlNGNULwG+DCyKk5zNVrbDtsND3w6357Atti2ezWyLbYuHvi22HZ5aO5yqyfb8kDQdkuzejgvcjeYXeVlVfWusvK1zIvDwqpp0wzPRcTu5CEmaw2yHJWnwbIvnHyeclGbeiiSLaDLfH+hpDI9O8gaa38vvM7nuW5M5riTp3myHJWnwbIvnGXs+SJIkSZKkTjnngyRJkiRJ6pTJB0mSJEmS1CmTD5IkSZIkqVMmHyRJkiRJUqdMPkiSJEmSpE6ZfJAkSZIkSZ36/wGZ7So7VoyPqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.evaluate_model(BaggingEnsemble(10, 0.9), 'Bagging')\n",
    "ensemble_handler.print_result()\n",
    "ensemble_handler.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baf75f9c5706d8d488ce868ad2ac828f",
     "grade": false,
     "grade_id": "qforest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Random Forest**\n",
    "\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features (columns).\n",
    "The rest of the implementation should be similar if not exactly the same as Bagging. In addition to keeping track of the estimators \n",
    "(in `RandomForest.estimators`, we also have to store the features indices that are used by each estimator (in `RandomForest.features_indices`).\n",
    "\n",
    "\n",
    "- 4.7 [4 points] First, complete `sample_data` to return a random sample of size `sample_ratio* len(X_train)` of labels and `feature_ratio * num_features` of features\n",
    "\n",
    "- 4.8 [4 points] Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data.\n",
    "Make sure to keep track of the sampled features for each estimator to use them in the prediction step\n",
    "\n",
    "- 4.9 [4 points] Complete `predict` method to return the most likely label by combining different estimators predictions. Instead of the majority vote used in KNNClassifier, you should use `predict_proba` method DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8742ff2833dd0e4749820747a33ce9",
     "grade": true,
     "grade_id": "aforest",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio=1.0, features_ratio=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.features_ratio = features_ratio\n",
    "        self.estimators = [] # to store the estimator\n",
    "        self.features_indices = [] # to store the feature indices used by each estimator\n",
    "\n",
    "    def sample_data(self, X_train, y_train):\n",
    "        X_sample, y_sample, features_indices = None, None, None\n",
    "        #Workspace 4.7\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train) and subset of features of size\n",
    "        #         features_ratio * num_features\n",
    "        #BEGIN \n",
    "        x_int = np.random.choice(X_train.shape[0], int(self.sample_ratio * len(X_train)))\n",
    "        X_sample = X_train[x_int, :]\n",
    "        y_sample = y_train[x_int]\n",
    "        features_indices = np.random.choice(X_train.shape[1], int(self.features_ratio * X_train.shape[1]))\n",
    "        #END\n",
    "        return X_sample, y_sample, features_indices\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        np.random.seed(42) # keep to have consistent results across run, you can change the value\n",
    "        for _ in range(self.n_estimators):\n",
    "            #Workspace 4.8\n",
    "            #TODO: sample data with random subset of rows and features using sample_data\n",
    "            #Hint: keep track of the features indices in features_indices to use in predict\n",
    "            #BEGIN \n",
    "            X_sample, y_sample, features_indices =  self.sample_data(X_train, y_train)\n",
    "            ith_weak_clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=11).fit(X_sample[:, features_indices], y_sample)\n",
    "            self.estimators.append(ith_weak_clf)\n",
    "            self.features_indices.append(features_indices)\n",
    "            #END\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #Workspace 4.9\n",
    "        #TODO: compute cumulative sum of predict proba from estimators and return the labels with highest likelihood\n",
    "        #BEGIN \n",
    "        nth_accu = []\n",
    "        most_likely_proba = []\n",
    "        for nth_obs in range(X_test.shape[0]):\n",
    "            for i in range(len(self.estimators)):\n",
    "                predicted_proba += self.estimators[i].predict_proba(X_test[:, self.features_indices[i]])\n",
    "                nth_accu.append(predicted_proba[nth_obs])\n",
    "            #cum_sum = np.cumsum(nth_accu, axis=0)\n",
    "            cum_sum = np.einsum('ij->j', nth_accu)\n",
    "            most_likely_proba.append(cum_sum)\n",
    "            nth_accu = []\n",
    "        answer = np.argmax(most_likely_proba, axis=1) \n",
    "        #END\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model  Precision(weighted)  Mean accuracy  Execution time\n",
      "3  RandomForest             0.700115       0.551020        2.066982\n",
      "1        Scikit             0.685739       0.680272        0.002921\n",
      "2       Bagging             0.659922       0.639456        0.137157\n",
      "0            WL             0.639790       0.625850        0.004021\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.evaluate_model(RandomForest(200, sample_ratio=0.7, features_ratio=0.1), 'RandomForest')\n",
    "ensemble_handler.print_result()\n",
    "ensemble_handler.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc8bad9492e27f8b3082e9657ad879ec",
     "grade": false,
     "grade_id": "qboosting",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Boosting**\n",
    "\n",
    "There are different methods of boosting, but we'll focus in this problem on Adaptive Boosting (AdaBoost).\n",
    "The logic of AdaBoost is to \"push\" each new learner to give more importance to previously misclassified data. We present\n",
    "below the multiclass variant of AdaBoost [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf). We denote $K$ the number of classes.\n",
    "\n",
    "AdaBosst is performed by increasing the weights of misclassified simple after each iteration:\n",
    "- Input: m samples $(X_i, y_i)_{i\\in [m]}$, number of boosting rounds $N$\n",
    "- Start with equal samples weights $W = (w_i), $ where   $w_i = \\frac{1}{\\texttt{n_samples}}$\n",
    "- at round j:\n",
    "    - Train estimator $h_j$ using current weights $W$\n",
    "    - Get the predicted $(\\hat{y}_i)$ on the training data using $h_j$\n",
    "    - Find the weighted error rate $\\epsilon_j$ using $W$: $\\epsilon_j=\\frac{\\sum_i w_i \\Delta(\\hat{y}_i, y_i)}{\\sum_i w_i}$\n",
    "    - Choose $\\alpha_j = \\log \\frac{1-\\epsilon_j}{\\epsilon_j} + \\log(K-1)$\n",
    "    - Update $W$ using: $w_i \\leftarrow w_i \\exp(\\alpha_j \\Delta(\\hat{y_i}, y_i)) $\n",
    "    - Normalize $W$ to have sum 1\n",
    "- Global estimator is $H = \\sum_j \\alpha_j h_j$,\n",
    "\n",
    "the $\\Delta$ function equals to 1 when the two argument are different, 0 otherwise.\n",
    "\n",
    "To understand how we implement $H$, imaging we have two classes, and we boosted for 3 rounds to get $(h_1, h_2, h_3)$,\n",
    "with weights $(\\alpha_1, \\alpha_2, \\alpha_3)$. When we want to predict the label of sample $x$, we get $(h_1(x), h_2(x), h_3(x)) = (0,1,0)$.\n",
    "\n",
    "In this case, label $0$ gets a weight $\\alpha_1+\\alpha_2$, while class $1$ get weight $\\alpha_2$. The predicted class is the one with\n",
    "the largest weight (1 if $\\alpha_2 > \\alpha_1 + \\alpha_3$, 0 otherwise)\n",
    "\n",
    "\n",
    "- 4.10 [6pts] Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on the same data but with different samples weights as detailed in the algorithm. Keep track of $(\\alpha_i)$\n",
    "\n",
    "_Hint: our weak learner (DecisionTreeClassifier) can take an argument `sample_weight` when calling the `fit` method, you'll have to use it to provide the weights $W$_\n",
    "\n",
    "- 4.11  [4 pts] Complete `predict` method to return the predicted label using the global estimator $H$. \n",
    "\n",
    "_Hint: use one hot encoding of the predicted labels from the weak learners and cumulate the prediction with weights $\\alpha_j$, a dictionary will also work_\n",
    "\n",
    "Notice that if the estimator is consistent (0 error rate on the training set), AdaBoost $\\alpha_j$ are no longer defined. That's why this method requires a **weak** learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc82033d238e3e9764c37a6e7dbf8bec",
     "grade": true,
     "grade_id": "aboosting",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.89318752  -9.48964457 -11.92288677 -10.46294145]\n",
      "[0.23333984 0.61251708 0.67085204 0.62710082]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdaBoost(object):\n",
    "\n",
    "    def __init__(self, n_estimators):\n",
    "        \"\"\"\n",
    "        :param n_estimators: number of estimators/ boosting rounds\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.num_classes = None\n",
    "        self.estimators = []\n",
    "        self.alphas = np.zeros(n_estimators)\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        self.num_classes = np.unique(y_train).shape[0] # K in the algorithm\n",
    "        weights = np.ones(len(X_train)) / len(X_train) # W in the algorithm\n",
    "        # Workspace 4.10\n",
    "        #TODO: Implement Multiclass Adaboost and keep track of the alpha_j\n",
    "        #BEGIN \n",
    "        k = self.num_classes\n",
    "        \n",
    "        for _ in range(self.n_estimators): \n",
    "            nth_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=11).fit(\n",
    "                                                X_train, y_train, sample_weight=weights)\n",
    "            # save estimators\n",
    "            self.estimators.append(nth_model)\n",
    "            y_hat = nth_model.predict(X_train)\n",
    "            # delta(y_hat, y)\n",
    "            indicator_y = np.where(y_hat == y_train, 1, 0)\n",
    "            # find weighter error \n",
    "            e_j = sum(weights*indicator_y)/np.einsum('i->', weights)\n",
    "            # compute epsilon log\n",
    "            e_log = np.log((1-e_j)/e_j)\n",
    "            # Calculating alpha of j \n",
    "            alpha_j = e_log + np.log(k-1)\n",
    "            # save alpha \n",
    "            self.alphas[_] = alpha_j\n",
    "            # update weights\n",
    "            weights = weights * np.exp(alpha_j*indicator_y)\n",
    "            # normalize weights\n",
    "            weights /= np.einsum('i->', weights)\n",
    "        #END\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        answer = 0\n",
    "        # Workspace 4.11\n",
    "        #TODO: get the labels returned by the global estimator defined as H\n",
    "        #Hint: Use one-hot format to accumulate alphas for different classes, or a dictionary\n",
    "        # The predicted label is the one that accumulates the largest sum of alphas\n",
    "        #Hint: We don't need predict_proba for this one\n",
    "        #BEGIN \n",
    "        h_j = []\n",
    "        for index, model in enumerate(self.estimators):\n",
    "            y_hat = model.predict(X_test)\n",
    "            \n",
    "        \n",
    "        #END\n",
    "        return answer\n",
    "ab = AdaBoost(2)\n",
    "ab.fit(X_bike, y_bike)\n",
    "ab.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_handler.evaluate_model(RandomForest(100, sample_ratio=0.8, features_ratio=0.8), 'RandomForest')\n",
    "ensemble_handler.evaluate_model(AdaBoost(40), 'AdaBoost')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff8b19b88620b8f625418ce00bd913c9",
     "grade": false,
     "grade_id": "q412",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Comparison**\n",
    "\n",
    "- 4.12 [4 points] Add different ensemble methods to the handler (try different parameters), plot, show, and compare them.\n",
    "What's the best weighted average precision we can get? What's the best accuracy? Which ensemble method achieves each of them?\n",
    "You can also compare to our best decision tree found in Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "803f91849b205987b919ea2138210a16",
     "grade": true,
     "grade_id": "a412a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(bike_sharing)\n",
    "ensemble_handler.evaluate_model(get_weak_learner(), 'weak_learner')\n",
    "#Workspace 4.12.a\n",
    "#TODO Add multiple instances of the ensemble methods. Plot and compare their performance\n",
    "#YOu can also add best tree from problem 3\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e67b7db837971f9c35f4cb8350852fe",
     "grade": true,
     "grade_id": "a412b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Write-up 4.12.b\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
